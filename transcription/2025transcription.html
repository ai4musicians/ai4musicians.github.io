<!DOCTYPE html>
<html lang="en" xml:lang="en">
    <head>
        <title>2025 Automatic Music Transcription Challenge</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
        <meta name="description" content="2025 Automatic Music Transcription Challenge" />
        <link rel="stylesheet" href="../assets/css/main.css" />
        <noscript>
            <link rel="stylesheet" href="../assets/css/noscript.css" />
        </noscript>
        <link rel="icon" type="image/x-icon" href="../assets/favicon.png" />
    </head>

    <body class="is-preload">
        <!-- Sidebar -->
        <section id="sidebar">
            <div class="inner">
                <nav>
                    <ul>
                        <li>
                            <a href="../index.html"><img src="../images/aim.png" width="50" alt="AIM Logo" /></a>
                        </li>
                        <li><a href="../index.html#news">News</a></li>
                        <li><a href="../index.html#amt">Our Projects</a></li>
                        <li><a href="../index.html#activities">Research Areas</a></li>
                        <li><a href="../index.html#who">Leaders</a></li>
                        <li><a href="../index.html#outreach">Outreach</a></li>
                        <li><a href="../index.html#vipteam">VIP Team</a></li>
                        <li><a href="../index.html#contact">Contact</a></li>
                    </ul>
                </nav>
            </div>
        </section>

        <!-- Wrapper -->
        <div id="wrapper">
            <!-- Main -->
            <section id="main" class="wrapper style1 fullscreen fade-up">
                <div class="inner">
                    <h1 class="major">2025 Automatic Music Transcription Challenge</h1>

                    <p class="subtitle">
                        Join the cutting-edge challenge to develop advanced music transcription models! <br />
                        <a
                            style="color: #f5f5f5"
                            href="https://forms.gle/sSofaBPKB6rZ84Jt8"
                            target="_blank"
                            rel="noopener noreferrer"
                        >
                            Register now for the 2025 Automatic Music Transcription Challenge!</a
                        >
                        <br />Submission window: April 1, 2025 - May 1, 2025
                    </p>
                    <!-- Summary -->

                    <section>
                        <h2>Summary</h2>
                        <p>
                            The 2025 Automatic Music Transcription (AMT) Challenge invites participants to develop
                            computer programs capable of accurately transcribing synthesized audio recordings of
                            classical music into Musical Instrument Digital Interface (MIDI) files. Each submission will
                            process 100 recordings, each up to 20 seconds long, within a maximum time limit of 4 hours.
                            The audio data has been synthesized to sound as realistic as possible, closely resembling
                            natural instrumental performances. Unlike previous challenges, participants will be informed
                            of the specific instruments present in each recording. While utilizing this information is
                            optional, incorrect instrument identification will incur a penalty, with smaller penalties
                            applied if the mistake involves similar instrument families. Evaluation criteria include the
                            accuracy of instrument identification, pitch, onset, offset, and dynamics.
                        </p>
                    </section>

                    <section id="leaderboard-section">
                        <a href="leaderboard.html" class="leaderboard-button button">
                            <span>Competition Leaderboard</span>
                        </a>
                    </section>

                    <section>
                        <h2>An Online Competition</h2>
                        <p>
                            The challenge is proudly sponsored by the
                            <a href="https://tc.computer.org/tcmc/"
                                >IEEE Technical Community on Multimedia Computing (TCMC)</a
                            >.
                        </p>
                    </section>

                    <section>
                        <h2>Technical Details</h2>
                        <p>
                            Participants will register on the official website
                            <a href="https://ai4musicians.org/">ai4musicians.org</a>, where sample music files,
                            including scores and audio recordings, will be provided to assist in model development.
                            Contestants may use any public or proprietary data for training their models. Submissions
                            will be open in April 2025, and each team's program will be executed on a GPU-equipped
                            system at Purdue's Rosen Center for Advanced Computing.
                        </p>
                        <p>
                            Teams may submit their models once every 24 hours, and a live leaderboard will display
                            performance results based on the sample data. Final rankings will be determined using an
                            additional set of holdout data. A sample open-source solution will be made available to
                            demonstrate input and output formats. Models performing worse than the sample will not be
                            eligible for awards. Winning teams will be invited to present their solutions at the 2025
                            IEEE ICME Conference.
                        </p>
                        <p>
                            For questions and updates, participants are encouraged to join the
                            <a
                                style="color: #f5f5f5"
                                href="https://join.slack.com/t/amt-s1z8510/shared_invite/zt-2yizumlvk-QoGklJqrTEv5HVqHs0gpvw"
                                target="_blank"
                                rel="noopener noreferrer"
                            >
                                AMT Slack Workspace</a
                            >.
                        </p>
                    </section>
                </div>
            </section>

            <section class="wrapper style6 fade-up">
                <div class="inner">
                    <section>
                        <h2>Submission Details</h2>
                        <p>
                            Participants who have registered for the competition must follow these guidelines for
                            submitting their models:
                        </p>
                        <h3>Repository Access</h3>
                        <p>
                            During registration, participants are required to provide a link to their code repository
                            along with a fine-grained access token. This token is necessary for the competition backend
                            to pull the model for execution.
                        </p>
                        <h3>Submission Branch</h3>
                        <p>
                            To submit a model, participants must create a branch titled <code>submission</code> in their
                            repository. The competition backend will automatically pull from this branch to run the
                            model.
                        </p>
                        <p>Submissions are validated if the following conditions are met:</p>
                        <ul>
                            <li>The branch titled <code>submission</code> exists.</li>
                            <li>New commits have been made to the branch since the last successful run.</li>
                        </ul>
                        <h3>Environment Configuration</h3>
                        <p>
                            Participants must include an <code>environment.yml</code> file in the root directory of
                            their GitHub repository. This file will be used to create a conda environment during the
                            model execution. Please ensure that the environment file correctly specifies all
                            dependencies and versions required to run the model.
                        </p>
                        <h3>Model Execution Requirements</h3>
                        <p>
                            To run the model, the repository must contain a file named <code>main.py</code> located in
                            the root directory. This script must accept the following command-line arguments:
                        </p>
                        <ul>
                            <li><code>-i</code>: Path to the input audio file (in .mp3 format).</li>
                            <li><code>-o</code>: Path to save the output MIDI file.</li>
                        </ul>
                        <p>Example usage:</p>
                        <pre><code>python main.py -i input.mp3 -o output.midi</code></pre>
                        <p>
                            All paths and directories referenced in the <code>main.py</code> file should be relative to
                            its own location. This ensures compatibility with the backend environment.
                        </p>
                        <h3>Input File Naming Convention</h3>
                        <p>
                            The input audio file names will include the MIDI instrument codes to specify the instruments
                            used in the recording. The MIDI instrument codes follow the General MIDI standard as
                            described
                            <a
                                href="https://www.ccarh.org/courses/253/handout/gminstruments/"
                                target="_blank"
                                rel="noopener noreferrer"
                                >here</a
                            >. The file name format will be as follows:
                        </p>
                        <p>Example:</p>
                        <pre><code>1._0_40_70.mp3</code></pre>
                        <p>
                            In this example, the numbers 0, 40, and 70 correspond to the MIDI instrument codes of the
                            instruments present in the audio file. Participants may choose to use this instrument
                            information or ignore it. However, if the model incorrectly identifies the instruments,
                            there will be a scoring penalty. The penalty is reduced if the mistake involves instruments
                            from similar families.
                        </p>
                        <h3>Submission Process</h3>
                        <p>
                            As long as the submission branch exists and new commits are detected, the system will
                            automatically run the model. The backend will handle the following:
                        </p>
                        <ul>
                            <li>Pulling the latest code from the <code>submission</code> branch.</li>
                            <li>Creating a conda environment using the provided <code>environment.yml</code> file.</li>
                            <li>Executing the model using the specified command.</li>
                            <li>Sending email notifications to the participant, including:</li>
                            <ul>
                                <li>Status updates on the model run.</li>
                                <li>Input and output of the model.</li>
                                <li>Performance statistics of the model.</li>
                                <li>
                                    The SLURM job output, allowing the participant to verify that the backend
                                    successfully processed their model.
                                </li>
                            </ul>
                        </ul>
                        <h3>Model Weights</h3>
                        <p>
                            Participants may use Git LFS (Large File Storage) for managing model weights within their
                            repository. The backend supports Git LFS, so there are no restrictions on using large files.
                        </p>
                    </section>
                </div>
            </section>

            <section class="wrapper style3 fade-up">
                <div class="inner">
                    <section>
                        <h2>Schedule</h2>
                        <p>
                            The following schedule outlines the key dates for the 2025 Automatic Music Transcription
                            Challenge. Please refer to these dates to stay on track throughout the competition.
                        </p>
                        <div class="schedule-container">
                            <div class="schedule-item">
                                <div class="schedule-date">11/20/2024</div>
                                <div class="schedule-event">
                                    Release of ten sample compositions from each contributing composer (available to
                                    contestants).
                                </div>
                            </div>
                            <div class="schedule-item">
                                <div class="schedule-date">12/15/2024</div>
                                <div class="schedule-event">Competition announcement.</div>
                            </div>
                            <div class="schedule-item">
                                <div class="schedule-date">12/31/2024</div>
                                <div class="schedule-event">
                                    Release of ten additional compositions from each composer (not available to
                                    contestants).
                                </div>
                            </div>
                            <div class="schedule-item">
                                <div class="schedule-date">01/31/2025</div>
                                <div class="schedule-event">Registration opens.</div>
                            </div>
                            <div class="schedule-item">
                                <div class="schedule-date">02/01/2025</div>
                                <div class="schedule-event">Sample solution release.</div>
                            </div>
                            <div class="schedule-item">
                                <div class="schedule-date">04/01/2025</div>
                                <div class="schedule-event">Submission window opens.</div>
                            </div>
                            <div class="schedule-item">
                                <div class="schedule-date">05/01/2025</div>
                                <div class="schedule-event">Submission window closes.</div>
                            </div>
                            <div class="schedule-item">
                                <div class="schedule-date">05/15/2025</div>
                                <div class="schedule-event">Winner announcement.</div>
                            </div>
                            <div class="schedule-item">
                                <div class="schedule-date">06/2025</div>
                                <div class="schedule-event">
                                    Presentation of winning solutions at the 2025 IEEE ICME conference.
                                </div>
                            </div>
                        </div>
                    </section>

                    <section>
                        <h2>Cash Awards</h2>
                        <p>The top-performing teams will be awarded cash prizes as follows:</p>
                        <ul>
                            <li>First Place: Up to $1,500 USD</li>
                            <li>Second Place: Up to $1,000 USD</li>
                            <li>Third Place: $500 USD</li>
                        </ul>
                        <p>
                            To receive the cash award, winners must open-source their solutions as specified in the
                            registration agreement. The submitted code must include adequate documentation to ensure
                            reproducibility. The organizers will conduct a thorough examination of the source code
                            before officially announcing the winners.
                        </p>
                        <p>
                            Please note that cash awards will only be provided to participants from countries not
                            subject to United States embargoes or sanctions. In some cases, the award may take the form
                            of travel grants, covering conference registration, hotel accommodations, and airfare.
                        </p>
                    </section>
                </div>
            </section>
            <section class="wrapper style4 fade-up">
                <div class="inner">
                    <section>
                        <h2>Sample Compositions</h2>
                        <p>
                            To provide participants with a clear understanding of the type of music their models will be
                            evaluated on, we are releasing 20 sample compositions. These compositions feature a diverse
                            range of instrumental arrangements, allowing participants to fine-tune their models for
                            optimal performance.
                        </p>
                        <p>The following instruments are included in the sample compositions:</p>
                        <ol>
                            <li>Piano</li>
                            <li>Violin</li>
                            <li>Cello</li>
                            <li>Flute</li>
                            <li>Bassoon</li>
                            <li>Trombone</li>
                            <li>Oboe</li>
                            <li>Viola</li>
                        </ol>
                        <p>
                            Each sample composition is provided as an MP3 audio file along with its corresponding sheet
                            music, available in both MIDI and PDF formats. These files serve as a representative dataset
                            to assist participants in developing and testing their models.
                        </p>
                        <p>
                            You can access the sample compositions here:
                            <a
                                href="https://drive.google.com/drive/folders/1p4YfcI7a17JZWxypOR-S8_-lqtFNZY7t?usp=sharing"
                                target="_blank"
                            >
                                Sample Compositions Google Drive Link
                            </a>
                        </p>
                    </section>

                    <section>
                        <h2>Sample Solution</h2>
                        <p>
                            To assist participants in developing their models, we provide a reference implementation of
                            the <strong>MT3</strong> (Multi-Task Multitrack Music Transcription) model. MT3, developed
                            by Google's Magenta team, is an advanced model designed to transcribe music involving
                            multiple instruments simultaneously. It utilizes a Transformer-based architecture to process
                            audio inputs and generate accurate musical notations.
                        </p>
                        <p>
                            To further support participants, we have created a dedicated repository that adapts MT3 for
                            the competition environment. This repository demonstrates how to set up and run the model
                            within the submission framework.
                        </p>
                        <p>You can access the MT3 competition-ready implementation here:</p>
                        <ul>
                            <li>
                                MT3 Competition-Ready Implementation:
                                <a href="https://github.com/ojas-chaturvedi/mt3-pytorch" target="_blank"
                                    >github.com/ojas-chaturvedi/mt3-pytorch</a
                                >
                            </li>
                            <li>
                                Original MT3 Repository by Google Magenta:
                                <a href="https://github.com/magenta/mt3" target="_blank">github.com/magenta/mt3</a>
                            </li>
                            <li>
                                MT3 Research Paper:
                                <a
                                    href="https://research.google/pubs/mt3-multi-task-multitrack-music-transcription/"
                                    target="_blank"
                                    >MT3: Multi-Task Multitrack Music Transcription</a
                                >
                            </li>
                        </ul>

                        <h3>Additional Sample Implementations</h3>
                        <p>
                            In addition to MT3, we provide implementations of two other models to offer a broader
                            perspective on music transcription approaches.
                        </p>
                        <ul>
                            <li>
                                <strong>Basic Pitch by Spotify:</strong> A model designed for robust pitch tracking and
                                transcription.
                                <ul>
                                    <li>
                                        Implementation:
                                        <a href="https://github.com/KayshavBhardwaj/basic-pitch-test-1" target="_blank"
                                            >github.com/KayshavBhardwaj/basic-pitch-test-1</a
                                        >
                                    </li>
                                    <li>
                                        Research Paper:
                                        <a href="https://arxiv.org/abs/2203.09893" target="_blank"
                                            >Basic Pitch: Spotify's Pitch Tracking Model</a
                                        >
                                    </li>
                                </ul>
                            </li>
                            <li>
                                <strong>ReconVAT:</strong> A model designed for semi-supervised music transcription
                                using variational autoencoders.
                                <ul>
                                    <li>
                                        Implementation:
                                        <a href="https://github.com/ojas-chaturvedi/ReconVAT" target="_blank"
                                            >github.com/ojas-chaturvedi/ReconVAT</a
                                        >
                                    </li>
                                    <li>
                                        Research Paper:
                                        <a href="https://arxiv.org/abs/2107.04954" target="_blank"
                                            >ReconVAT: Semi-Supervised Learning for Music Transcription</a
                                        >
                                    </li>
                                </ul>
                            </li>
                        </ul>
                        <p>
                            Participants are encouraged to study these implementations as foundational models to gain
                            insights into effective music transcription architectures. They serve as valuable starting
                            points for developing and refining your own transcription models.
                        </p>
                        <p>
                            <strong>Note:</strong> To qualify as a winning submission, your model must achieve a
                            transcription accuracy score higher than MT3. This criterion ensures that new models
                            demonstrate meaningful progress in the field of music transcription.
                        </p>
                    </section>
                </div>
            </section>
            <section class="wrapper style2 fade-up">
                <div class="inner">
                    <section>
                        <h2>Organizers</h2>
                        <p>
                            The 2025 Automatic Music Transcription Challenge is organized by leading experts from
                            multiple institutions, dedicated to advancing research in music transcription and
                            computational musicology.
                        </p>
                        <table class="modern-table">
                            <tr>
                                <th>Photo</th>
                                <th>Name</th>
                                <th>Email</th>
                                <th>Organization</th>
                            </tr>
                            <tr>
                                <td>
                                    <img
                                        src="https://www.cla.purdue.edu/directory/images/kristen-yeon-ji-yun.jpg"
                                        alt="Kristen Yeon Ji Yun"
                                        class="profile-img"
                                    />
                                </td>
                                <td>Kristen Yeon-Ji Yun</td>
                                <td>yun98@purdue.edu</td>
                                <td>Department of Music, Purdue University</td>
                            </tr>
                            <tr>
                                <td>
                                    <img
                                        src="https://engineering.purdue.edu/ResourceDB/ResourceFiles/image277153"
                                        alt="Yung-Hsiang Lu"
                                        class="profile-img"
                                    />
                                </td>
                                <td>Yung-Hsiang Lu</td>
                                <td>yunglu@purdue.edu</td>
                                <td>School of Electrical and Computer Engineering, Purdue University</td>
                            </tr>
                            <tr>
                                <td>
                                    <img
                                        src="https://gkt.sh/content/images/size/w960/2024/06/George_Thiruvathukal-2-1.jpg"
                                        alt="George Thiruvathukal"
                                        class="profile-img"
                                    />
                                </td>
                                <td>George K. Thiruvathukal</td>
                                <td>gthiruvathukal@luc.edu</td>
                                <td>Department of Computer Science, Loyola University Chicago</td>
                            </tr>
                            <tr>
                                <td>
                                    <img
                                        src="https://www.cla.purdue.edu/directory/images/tae-hong-park.jpg"
                                        alt="Tae Hong Park"
                                        class="profile-img"
                                    />
                                </td>
                                <td>Tae Hong Park</td>
                                <td>thp@purdue.edu</td>
                                <td>Department of Music, Purdue University</td>
                            </tr>
                            <tr>
                                <td>
                                    <img
                                        src="https://www.cla.purdue.edu/directory/images/harry-bulow.jpg"
                                        alt="Harry Bulow"
                                        class="profile-img"
                                    />
                                </td>
                                <td>Harry Bulow</td>
                                <td>hbulow@purdue.edu</td>
                                <td>Department of Music, Purdue University</td>
                            </tr>
                            <tr>
                                <td>
                                    <img src="../images/ojaschaturvedi.jpg" alt="Ojas Chaturvedi" class="profile-img" />
                                </td>
                                <td>Ojas Chaturvedi</td>
                                <td>ochaturv@purdue.edu</td>
                                <td>Department of Computer Science, Purdue University</td>
                            </tr>
                            <tr>
                                <td>
                                    <img
                                        src="../images/kayshavbhardwaj.jpg"
                                        alt="Kayshav Bhardwaj"
                                        class="profile-img"
                                    />
                                </td>
                                <td>Kayshav Bhardwaj</td>
                                <td>bhardw43@purdue.edu</td>
                                <td>Department of Liberal Arts, Purdue University</td>
                            </tr>
                        </table>
                    </section>
                    <section>
                        <h2>Contributing Composers</h2>
                        <p>
                            The following composers have contributed music to the challenge, helping to establish a
                            diverse and realistic dataset for model training and evaluation.
                        </p>
                        <table class="modern-table">
                            <tr>
                                <th>Photo</th>
                                <th>Name</th>
                                <th>Email</th>
                                <th>Organization</th>
                            </tr>
                            <tr>
                                <td>
                                    <img
                                        src="https://www.cla.purdue.edu/directory/images/harry-bulow.jpg"
                                        alt="Harry Bulow"
                                        class="profile-img"
                                    />
                                </td>
                                <td>Harry Bulow</td>
                                <td>hbulow@purdue.edu</td>
                                <td>Department of Music, Purdue University</td>
                            </tr>
                            <tr>
                                <td>
                                    <img
                                        src="https://www.cla.purdue.edu/directory/images/tae-hong-park.jpg"
                                        alt="Tae Hong Park"
                                        class="profile-img"
                                    />
                                </td>
                                <td>Tae Hong Park</td>
                                <td>thp@purdue.edu</td>
                                <td>Department of Music, Purdue University</td>
                            </tr>
                            <tr>
                                <td>
                                    <img
                                        src="https://huberthowe.org/wp-content/uploads/2023/02/Tuck.12.21.22b-300x293.jpg"
                                        alt="Hubert Howe"
                                        class="profile-img"
                                    />
                                </td>
                                <td>Hubert Howe</td>
                                <td>hubert.howe@gmail.com</td>
                                <td>Juilliard School of Music (retired)</td>
                            </tr>
                            <tr>
                                <td>
                                    <img
                                        src="https://music.utahtech.edu/wp-content/uploads/sites/28/2019/11/ka-wai_yu200x250.jpg"
                                        alt="Ka-Wai Yu"
                                        class="profile-img"
                                    />
                                </td>
                                <td>Ka-Wai Yu</td>
                                <td>ka-wai.yu@utahtech.edu</td>
                                <td>Department of Music, Utah Tech University</td>
                            </tr>
                        </table>
                    </section>

                    <section>
                        <h2>For Contributing Composers</h2>
                        <p>
                            The 2025 challenge features invited composers to ensure the controlled complexity of the
                            musical data. The objective is to produce high-quality compositions that allow meaningful
                            evaluation of automatic transcription models.
                        </p>
                        <p>
                            Composers retain the copyright of their works while granting royalty-free, non-exclusive
                            rights to the challenge organizers for redistribution and analysis. The organizers may
                            modify compositions to meet scoring requirements.
                        </p>
                        <p>
                            Each invited composer is expected to contribute between 10 to 30 pieces, each approximately
                            20 seconds long, and divided into three difficulty levels: easy, medium, and difficult.
                        </p>
                        <p>Guidelines for Composition:</p>
                        <ul>
                            <li>Tempo: 60-90 bpm</li>
                            <li>Pitch Range: C2 to C7</li>
                            <li>Smallest rhythmic duration: sixteenth-notes/rests</li>
                            <li>No swing rhythms; use precise notation</li>
                            <li>No doubly-dotted notes</li>
                            <li>No trills or mordents</li>
                            <li>Meters: 3/4, 4/4, 6/8</li>
                            <li>Dynamic range: pp to ff</li>
                            <li>
                                Up to three distinct instruments per composition, avoiding combinations like violin and
                                cello due to similar timbre
                            </li>
                            <li>Submit files in PDF (score), MusicXML, and MIDI formats</li>
                        </ul>
                        <p>The selected instrument set is as follows:</p>
                        <ol>
                            <li>Piano</li>
                            <li>Violin</li>
                            <li>Cello</li>
                            <li>Flute</li>
                            <li>Bassoon</li>
                            <li>Trombone</li>
                            <li>Oboe</li>
                            <li>Viola</li>
                        </ol>
                    </section>
                </div>
            </section>
            <section class="wrapper style7 fade-up">
                <div class="inner">
                    <section>
                        <h2>Frequently Asked Questions</h2>
                        <table class="modern-table">
                            <tr>
                                <th>Question</th>
                                <th>Answer</th>
                            </tr>
                            <tr>
                                <td>Will a leaderboard be provided?</td>
                                <td>Yes.</td>
                            </tr>
                            <tr>
                                <td>
                                    If my team is No. 1 in the leaderboard, does that mean my team will be No. 1 in the
                                    final winner announcement?
                                </td>
                                <td>
                                    Maybe yes, maybe not. The final winners will be determined by a set of holdout data
                                    that is similar to, but different from, the public sample data.
                                </td>
                            </tr>
                            <tr>
                                <td>
                                    How can it be possible that No.1 in the leaderboard is not No.1 in the final
                                    announcement?
                                </td>
                                <td>
                                    It is possible if the solution overfits the public data but does not generalize well
                                    for the holdout data.
                                </td>
                            </tr>
                            <tr>
                                <td>Will the organizers provide training data?</td>
                                <td>
                                    The competition will provide sample data, not training data. Contestants can use any
                                    data (including public and proprietary data).
                                </td>
                            </tr>
                            <tr>
                                <td>Can industry participate?</td>
                                <td>Yes.</td>
                            </tr>
                            <tr>
                                <td>Can industry partner with academia?</td>
                                <td>Yes.</td>
                            </tr>
                            <tr>
                                <td>Is open-source required?</td>
                                <td>Open-source is required for winners before receiving the cash awards.</td>
                            </tr>
                            <tr>
                                <td>Can I participate and receive the ranking, without open-source?</td>
                                <td>
                                    You can participate without open-source if you are not a winner. If you are a winner
                                    and do not open-source, you will not receive any cash award.
                                </td>
                            </tr>

                            <tr>
                                <td>What is the required license for the winners solutions?</td>
                                <td>The winners decide their licenses.</td>
                            </tr>
                            <tr>
                                <td>Can winners submit papers about their solutions?</td>
                                <td>
                                    Yes, certainly. The organizers are also seeking the opportunities of a special issue
                                    in a journal.
                                </td>
                            </tr>
                            <tr>
                                <td>Do winners have to present at ICME?</td>
                                <td>Winners are invited to present at 2025 ICME but this is not a requirement.</td>
                            </tr>
                        </table>
                    </section>
                </div>
            </section>
        </div>

        <!-- Scripts -->
        <script src="../assets/js/jquery.min.js"></script>
        <script src="../assets/js/jquery.scrollex.min.js"></script>
        <script src="../assets/js/jquery.scrolly.min.js"></script>
        <script src="../assets/js/browser.min.js"></script>
        <script src="../assets/js/breakpoints.min.js"></script>
        <script src="../assets/js/util.js"></script>
        <script src="../assets/js/main.js"></script>
        <script src="leaderboard/preloadLeaderboard.js"></script>
    </body>
</html>
