<!DOCTYPE html>
<html lang="en">
  <head>
    <title>AI (Artificial Intelligence) for Musicians</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" /> -->
    <link rel="stylesheet" href="assets/css/new_main.css" />
    <link rel="stylesheet" href="assets/css/fontawesome-all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css"
    />
  </head>
  <body>
    <div class="navbar">
      <a href="#intro">
        <div class="navlogo">
          <img src="images/aim.png" alt="aim_logo" />
        </div>
      </a>
      <div class="subnavbar">
        <h4>Artificial Intelligence for Musicians Research Group</h4>
        <hr />
        <div class="links">
          <a href="#evaluator">Our Projects</a>
          <a href="#activities">Research Areas</a>
          <a href="#who">Leaders</a>
          <a href="#outreach">Outreach</a>
          <a href="#vipteam">VIP Team</a>
          <a href="#contact">Contact</a>
        </div>
      </div>
    </div>

    <!-- <hr class="navhr" /> -->
    <main>
      <div class="hero">
        <div class="inner">
          <!---		<img src="images/aim.png" style="width:100%" alt="" data-position="top center" /> --->
          <h1>Welcome to AIM.</h1>
          <p>
            <b>AIM (Artificial Intelligence for Musicians) </b> is a Purdue
            <b>music technology research group</b>, whose aim is to create
            reactive, human-like systems which support musicians during their
            practice sessions and performances.
          </p>
          <p>
            Some of AIM's projects are supported by
            <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2326198"
              >a National Science Foundation grant</a
            >.
          </p>
          <p>
            We are looking for motivated graduate or undergraduate students to
            join our team. <a href="vip_info.html">Click here for info.</a>
          </p>
          <iframe
            src="https://ghbtns.com/github-btn.html?user=Purdue-Artificial-Intelligence-in-Music&type=follow&count=true&size=large"
            frameborder="0"
            scrolling="0"
            width="500"
            height="30"
            title="GitHub"
          ></iframe>
          <!---
	    <ul class="actions">
	      <li><a href="#one" class="button scrolly">Learn more</a></li>
	    </ul>
	    --->
        </div>
        <div class="news">
          <h3>News</h3>
          <ul>
            <li>
              <a href="2025robot.html">Workshop "Robotic Musician" in 2025 </a>
            </li>
            <li>
              <a href="2025aaai.html">Workshop "AI for Music" in 2025 AAAI </a>
            </li>
            <li>
              <a href="transcription/2025transcription.html"
                >2025 Transcription Competition
              </a>
            </li>
          </ul>
        </div>
      </div>

      <!-- Evaluator -->
      <div class="research-area" id="evaluator">
        <div class="research-image">
          <img
            src="images/evaluator.jpg"
            alt="evaluator"
            data-position="top center"
          />
        </div>
        <div class="research-content">
          <h2>
            <a
              href="https://github.com/Purdue-Artificial-Intelligence-in-Music/Evaluator-code"
              >Evaluator</a
            >
          </h2>
          <h4>Fall 2023 - present</h4>
          <p>
            Evaluator is an app that aims to help musicians practice more
            effectively. It utilizes computer vision and YOLO localization
            techniques to help musicians track, analyze, and improve their
            posture. It also uses spectrogram analysis and multi-modal
            transformers to help the musicians identify their mistakes in music
            and correct them.
          </p>
          Drawing by Cecilia Ines Sanchez.

          <!---
			<p>This project will develop and integrate techniques to create two AI-enabled tools to support string music performers. The first tool, the Evaluator, aims to improve individual practice and performance. It analyzes a musician’s sound and compares it to digitized music scores to detect deviations in intonation, rhythm, and dynamics and suggest better posture based on sample performers’ recording with correct posture. The second tool, the Companion, plays the part of one or several instruments to replace absent musicians with matching tempo, and style of the human musicians through audio analysis of their performance while also responding in real-time to verbal instructions. </p>
			--->
        </div>
      </div>

      <!-- Companion -->
      <div class="research-area" id="companion">
        <div href="#" class="research-image">
          <img
            src="images/companion.png"
            alt="companion"
            data-position="top center"
          />
        </div>
        <div class="research-content">
          <h2>
            <a
              href="https://github.com/Purdue-Artificial-Intelligence-in-Music/Companion-code"
              >Companion</a
            >
          </h2>
          <h4>Fall 2023 - present</h4>
          <p>
            Companion is an app that not only plays along with a human player
            during a chamber music piece, but actively responds to their playing
            habits and voice commands like a real human would. The project
            involves machine learning and filtering/DSP algorithms to analyze
            and edit sound quickly and accurately and utilizes small NLP
            language models for voice command implementation.
          </p>
          <p>Drawing by Cecilia Ines Sanchez.</p>
        </div>
      </div>

      <!-- Visualize -->
      <div class="research-area" id="mus2vid">
        <div href="#" class="research-image">
          <img
            src="images/visualizemusic.png"
            alt="visualizemusic"
            data-position="top center"
          />
        </div>
        <div class="research-content">
          <h2>
            <a
              href="https://github.com/Purdue-Artificial-Intelligence-in-Music/Mus2Vid-code"
              >Mus2Vid</a
            >
          </h2>
          <h4>Spring 2022 - present</h4>
          <p>
            Mus2Vid is a real-time art project that uses diffusion models to
            generate video depictions in response to classical music. It uses
            recurrent and transformer networks to analyze input audio and
            estimate its emotion and genre qualities, which are converted into
            text and fed to a text-to-image diffusion model to generate images.
          </p>

          <ul class="actions">
            <li>
              <a href="visualize.html" class="button scrolly">Learn more</a>
            </li>
          </ul>
        </div>
      </div>

      <!-- Robot Cello -->

      <div class="research-area" id="robotcello">
        <div class="research-image">
          <iframe
            src="https://www.youtube.com/embed/Rf0bSJr8yFM?si=6LKA8egAhXsrftlh"
            title="YouTube video player"
            frameborder="0"
            height="100%"
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
            allowfullscreen
          ></iframe>
        </div>

        <div class="research-content" id="robotcello">
          <h2>Robot Cello</h2>
          <h4>Spring 2024 - present</h4>
          <p>
            As the name suggests, Robot Cello is a project about using
            reinforcement learning to teach a robot arm to play cello. The
            project is currently in its survey phase but is currently
            investigating using motion capture technology to get training data
            for an RL model.
          </p>
          <p>
            We partner with the
            <a href="https://www.rcac.purdue.edu/envision"
              >Purdue Envision Center</a
            >
            to collect motion data for our robot arm to train on. On the left is
            a video of Prof. Yun playing cello while wearing a motion-capture
            rig.
          </p>
        </div>
      </div>

      <div id="activities" class="inner">
        <h2>Research Areas + Questions</h2>
        <p>
          Our projects often span most or all of these areas, as they are all
          important to making effective, human-like music technology.
        </p>
        <div class="features">
          <section>
            <span class="fas icon fas fa-volume-up"></span>
            <h3>Generative Audio and DSP</h3>
            <p>
              How can Companion utilize machine learning and filtering to
              resynthesize string instrument articulations on-the-fly?
            </p>
          </section>
          <section>
            <span class="fas icon fas fa-stopwatch"></span>
            <h3>Beat Detection and Tempo Tracking</h3>
            <p>
              What are the most effective methods for Companion, Evaluator, and
              Mus2Vid to follow a musician's playing in reference to a score,
              and play along to match?
            </p>
          </section>
          <section>
            <span class="fas icon fas fa-eye"></span>
            <h3>Emotion and Perception</h3>
            <p>
              How can Mus2Vid analyze emotion of classical music in real-time
              and utilize it to generate real-time video accompaniments?
            </p>
          </section>
          <section>
            <span class="fas icon fas fa-music"></span>
            <h3>Music Classification/Information Retrieval</h3>
            <p>
              What musical features extracted from various media, such as tempo,
              key, genre, notes, are useful to music performance technology, and
              how can we extract such features?
            </p>
          </section>
          <section>
            <span class="fas icon fas fa-users"></span>
            <h3>Human-Computer Interaction</h3>
            <p>
              How can our apps be designed in ways that are human-like and
              natural for humans to interact with?
            </p>
          </section>
          <section>
            <span class="fas icon fas fa-comments"></span>
            <h3>User Studies + Deployments</h3>
            <p>
              How can we ensure that our users actually utilize and enjoy the
              apps we develop?
            </p>
          </section>
        </div>
        <!---
	  <ul class="actions">
	    <li><a href="generic.html" class="button">Learn more</a></li>
	  </ul>
	  --->
      </div>

      <!-- who -->
      <section id="who" class="wrapper style5 spotlights">
        <section>
          <div class="content">
            <div class="inner">
              <h2>Meet the team!</h2>
              <img
                src="images/aim_photo.jpg"
                style="width: 100%"
                alt=""
                data-position="top center"
              />
              <p>
                Our team comes from a wide variety of backgrounds, including the
                Purdue Colleges of Engineering, Science, Liberal Arts,
                Management, and more. We have a mix of Purdue professors,
                graduate students, and undergraduate students leading our
                projects and research efforts.
              </p>
              <a href="the_team.html" class="button scrolly">Learn more</a>
            </div>
          </div>
        </section>
      </section>

      <section id="outreach" class="wrapper style3 fade-up">
        <div class="inner">
          <h2>Outreach</h2>
          <p>
            Here are some of the ways AIM ensures that it is integrated with
            both Purdue's local community and the global community.
          </p>
          <div class="features">
            <section>
              <span class="fas icon solid major fa-school"></span>
              <h3>Vertically Integrated Project</h3>
              <p>
                AIM has an associated Vertically Integrated Project, which
                enables research experiences for Purdue undergraduates.
              </p>
              <ul class="actions">
                <li>
                  <a href="#vipteam" class="button scrolly">Learn More</a>
                </li>
              </ul>
            </section>
            <section>
              <span class="fas icon solid major fa-microscope"></span>
              <h3>Multidisciplinary Research</h3>
              <p>
                Music technology is a field with very multidisciplinary problems
                and solutions. As such, we draw from a wide variety of
                departments for our talent, including Music, Electrical and
                Computer Engineering, Computer Science, Technology, Art, Design,
                and Management.
              </p>
            </section>
            <section>
              <span class="fas icon solid major fa-comments"></span>
              <h3>User Studies</h3>
              <p>
                AIM actively runs user studies on the tools that are developed
                to ensure that users can figure out how to use them and enjoy
                them.
              </p>
            </section>
            <section>
              <span class="fas icon solid major fa-ticket-alt"></span>
              <h3>Presentations/Concerts</h3>
              <p>
                AIM members have given speeches, presentations, and concerts
                about and using AIM technology around the world.
              </p>
            </section>
          </div>
          <!---
	  <ul class="actions">
	    <li><a href="generic.html" class="button">Learn more</a></li>
	  </ul>
	  --->
        </div>
      </section>

      <!-- VIP Team -->
      <section id="vipteam" class="wrapper style2 fullscreen fade-up">
        <div class="inner">
          <!---<img src="images/vip-web.jpg" style="width:100%" /> --->
          <h1>Vertically Integrated Projects Team</h1>

          <p>
            AIM stands out among other music technology research groups because
            of its pedagogy. While other music technology groups may cater
            primarily to graduate students and professionals, our group is open
            to all Purdue students of any major and experience level.
          </p>
          <p>
            We hope that by helping any student interested in music
            technology/machine learning learn to work with these technologies,
            we can make a difference in these students' lives while
            simultaneously encouraging the adoption of music technology.
          </p>
          <a href="vip_info.html" class="button scrolly">Learn more</a>

          <a href="#" class="research-image"
            ><img
              src="images/companionteam20240125.jpg"
              style="width: 100%"
              alt="team"
              data-position="fit"
          /></a>
        </div>
      </section>

      <!-- Contact -->
      <section id="contact" class="wrapper style5 fade-up">
        <div class="inner">
          <!--- <img src="images/aim.png" style="width:100%" alt="" data-position="top center" /> --->
          <h2>Get in touch</h2>
          <div class="split style1">
            <section>
              <ul class="contact">
                <li>
                  <h3>Email</h3>
                  General inquiry:
                  <a href="mailto:yun98@purdue.edu">yun98@purdue.edu</a>
                  <br />VIP team-related inquiries:
                  <a href="mailto:yunglu@purdue.edu">yunglu@purdue.edu</a>
                </li>
                <li>
                  <h3>Phone</h3>
                  <span>(765) 494-0973</span>
                </li>
                <li>
                  <h3>Social</h3>
                  <a
                    href="https://www.linkedin.com/in/kristen-yeon-ji-yun-72112591/"
                    class="link"
                  >
                    <span
                      class="fab icon solid major brands fa-linkedin"
                    ></span>
                    <span class="link">LinkedIn</span></a
                  >
                </li>
              </ul>
            </section>
          </div>
        </div>
      </section>
    </main>

    <!-- Footer -->
    <footer id="footer">
      <p>&copy; Purdue AIM 2024. All rights reserved.</p>
    </footer>
  </body>
</html>
