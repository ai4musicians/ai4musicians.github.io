<!DOCTYPE html>
<html lang="en" xml:lang="en">
    <head>
        <title>2025 ICME Workshop AI for Music</title>
        <meta charset="utf-8" />
        <meta
            name="viewport"
            content="width=device-width, initial-scale=1, user-scalable=no"
        />
        <meta
            name="description"
            content="Join the 2025 ICME Workshop on Artificial Intelligence for Music, exploring the intersection of AI and music creation, recognition, education, and more."
        />
        <link rel="stylesheet" href="assets/css/main.css" />
        <noscript>
            <link rel="stylesheet" href="assets/css/noscript.css" />
        </noscript>
        <link rel="icon" type="image/x-icon" href="/assets/favicon.png" />
    </head>

    <body class="is-preload">
        <!-- Sidebar -->
        <section id="sidebar">
            <div class="inner">
                <nav>
                    <ul>
                        <li>
                            <a href="index.html"
                                ><img
                                    src="images/aim.png"
                                    width="50"
                                    alt="AIM Logo"
                            /></a>
                        </li>
                        <li><a href="index.html#news">News</a></li>
                        <li><a href="index.html#amt">Our Projects</a></li>
                        <li>
                            <a href="index.html#activities">Research Areas</a>
                        </li>
                        <li><a href="index.html#who">Leaders</a></li>
                        <li><a href="index.html#outreach">Outreach</a></li>
                        <li><a href="index.html#vipteam">VIP Team</a></li>
                        <li><a href="index.html#contact">Contact</a></li>
                        <li><a href="#call-for-papers">Call for Papers</a></li>
                        <!-- Added Navigation Link -->
                    </ul>
                </nav>
            </div>
        </section>

        <!-- Wrapper -->
        <div id="wrapper">
            <!-- Main -->
            <section id="main" class="wrapper style1 fullscreen fade-up">
                <div class="inner">
                    <h1 class="major">Artificial Intelligence for Music</h1>

                    <p class="subtitle">
                        A workshop at
                        <a
                            href="https://2025.ieeeicme.org/"
                            target="_blank"
                            rel="noopener noreferrer"
                            >2025 ICME Annual Conference</a
                        >
                    </p>

                    <!-- Summary -->

                    <section>
                        <h2>Date: 2025/06/30 Monday</h2>
                        <h2>Workshop Summary</h2>
                        <p>
                            Music is an essential component of multimedia
                            content. This workshop will explore the dynamic
                            intersection of artificial intelligence and music.
                            This workshop investigates how AI is changing the
                            music industry and education, from composition to
                            performance, production, collaboration, and audience
                            experience. Participants will gain insights into the
                            ways AI can enhance creativity and enable musicians
                            and producers to push the boundaries of their art.
                            The workshop will also discuss AI's impacts on music
                            education and the careers of musicians. We will
                            cover topics such as AI-driven music composition,
                            where algorithms generate melodies, harmonies, and
                            even full orchestral arrangements.
                            Computer-generated music may be combined with
                            computer-generated video to create the entire
                            multimedia content. The workshop will discuss how AI
                            tools can assist in sound design, remixing, and
                            mastering, allowing for new sonic possibilities and
                            efficiencies in music production. Additionally, the
                            workshop will discuss the legal and ethical
                            implications of AI in music, including questions of
                            authorship, originality, and the role of the human
                            artist in an increasingly automated world. This
                            workshop is designed for AI researchers, musicians,
                            producers, and educators interested in the status
                            and future of AI in music. The organizing team will
                            hold a competition for Automatic Music Transcription
                            (AMT). This online competition will accept
                            submissions worldwide, including both academia and
                            industry. The winners will present their solutions
                            at this ICME workshop. This competition is sponsored
                            by the IEEE Technical Community on Multimedia
                            Computing (TCMC) and the Computer Society. More
                            details about this challenge will be available
                            <a
                                href="https://ai4musicians.org/transcription/2025transcription.html"
                                >here</a
                            >.
                        </p>
                    </section>

                    <!-- Call for Papers -->
                    <section id="call-for-papers">
                        <div class="inner">
                            <h2>Call for Papers</h2>

                            This one-day workshop will explore the dynamic
                            intersection of artificial intelligence and
                            multimedia with an emphasis on music and audio
                            technologies. The workshop explores how AI is
                            transforming music creation, recognition, and
                            education, ethical and legal implications, as well
                            as business opportunities. We will investigate how
                            AI is changing the music industry and educationâ€”from
                            composition to performance, production,
                            collaboration, and audience experience. Participants
                            will gain insights into the technological challenges
                            in music and how AI can enhance creativity, enabling
                            musicians and producers to push the boundaries of
                            their art. The workshop will cover topics such as
                            AI-driven music composition, where algorithms
                            generate melodies, harmonies, and even full
                            orchestral arrangements. We will discuss how AI
                            tools assist in sound design, remixing, and
                            mastering, allowing for new sonic possibilities and
                            efficiencies in music production. Additionally,
                            we'll examine AI's impact on music education and the
                            careers of musicians, exploring advanced learning
                            tools and teaching methods. AI technologies are
                            increasingly adopted in the music and entertainment
                            industry. The workshop will also discuss the legal
                            and ethical implications of AI in music, including
                            questions of authorship, originality, and the
                            evolving role of human artists in an increasingly
                            automated world. This workshop is designed for AI
                            researchers, musicians, producers, and educators
                            interested in the current status and future of AI in
                            music.

                            <h3>Topics of Interest</h3>

                            Topics of Interest include, but are not limited to

                            <ul>
                                <li>
                                    AI-Driven Music Composition and Generation
                                </li>
                                <li>AI in Music Practice and Performance</li>
                                <li>
                                    AI-based Music Recognition and Transcription
                                </li>
                                <li>AI Applications in Sound Design</li>
                                <li>AI-Generated Videos to Accompany Music</li>
                                <li>AI-Generated Lyrics Based on Music</li>
                                <li>
                                    Legal or Ethical Implications of AI on Music
                                </li>
                                <li>AI's Impacts on Musicians' Careers</li>
                                <li>AI Assisted Music Education</li>
                                <li>Business Opportunities of AI and Music</li>
                                <li>Music Datasets and Data Analysis</li>
                            </ul>

                            <h3>Submission Requirements</h3>
                            <p>
                                Please follow the submission requirements of
                                <a
                                    href="https://2025.ieeeicme.org/author-information-and-submission-instructions/"
                                    >ICME 2025</a
                                >. Papers must be no longer than 6 pages,
                                including all text, figures, and references.
                                This workshop will follow ICME submission and
                                adopt <i>double blind</i> reviews. Authors
                                should not identify themselves in the submitted
                                PDF files.
                            </p>

                            <p>
                                Work in progress is welcome. Authors are
                                encouraged to include descriptions of their
                                prototype implementations. Additionally, authors
                                are encouraged to interact with workshop
                                attendees by including posters or demonstrations
                                at the end of the workshop. Conceptual designs
                                without any evidence of practical implementation
                                are discouraged.
                            </p>

                            <p>
                                The authors agree that their papers submitted to
                                this workshop have not been previously published
                                (or accepted) in substantially similar forms.
                                Furthermore, authors should not submit any
                                papers that contain significant overlap with any
                                papers that are being reviewed by a conference
                                or a journal.
                            </p>

                            Submit papers to
                            <a
                                href="https://cmt3.research.microsoft.com/ICMEW2025"
                                >CMT</a
                            >.

                            <h3>Important Dates</h3>
                            <ul>
                                <li>
                                    <strong>Submission Deadline:</strong>
                                    April 1, 2025 (11:59PM Pacific Time)
                                </li>
                                <li>
                                    <strong>Notification of Acceptance:</strong>
                                    April 25, 2025
                                </li>
                                <li>
                                    <strong>Final Version Due:</strong> May 15,
                                    2025
                                </li>
                            </ul>

                            <p>
                                Accepted papers will be posted on the workshop
                                website and IEEEXplore.
                            </p>
                        </div>
                    </section>

                    <!-- Schedule -->
                    <section>
                        <h2>Workshop Schedule</h2>

                        <table class="modern-table", text-align: left>
                            <tr>
                                <th>Time</th> 
                               <th>Topic</th>
                            </tr>
                            <tr>
                                <td>08:30AM</td>
                                <td>Welcome by Organizers: Yung-Hsiang Lu and Yeon-Ji Yun</td>
                            </tr>
                            <tr>
                                <td>08:40AM</td>
                                <td>Keynote Speech by Zhiyao Duan. Moderator: Yeon-Ji Yun</td>
                            </tr>

                            <tr>
                                <td>09:30AM</td>
                                <td>Invited Speech by Fatemeh Jamshidi. Moderator: Yeon-Ji Yun</td>
                            </tr>
                            <tr>
                                <td>10:10AM</td>
                                <td>Break</td>
                            </tr>

                            <tr>
                                <td>10:20AM</td>
                                <td>Invited Speech by Gus Xia. Moderator: Emmanouil Benetos</td>
                            </tr>

                            <tr>
                                <td>11:00AM</td>
                                <td> Invited Speech by Geoffroy Peeters. Moderator: Emmanouil Benetos</td>
                                </td>
                            </tr>

                            <tr>
                                <td>11:40PM</td>
                                <td>Discussion with the Morning Speakers. Moderator: Emmanouil Benetos</td>
                            </tr>
                            <tr>
                                <td>12:00PM</td>
                                <td>Lunch Break</td>
                            </tr>

                            <tr>
                                <td>01:00PM</td>
                                <td>Invited Speech by Emmanouil Benetos. Moderator: Zhiyao Duan</td>
                            </tr>

                            <tr>
                                <td>01:40PM</td>
                                <td> Paper Presentations. Moderator: Zhiyao Duan
				  <p align="left">
				  <ol>
				    <li> Analysis of Improvised Jazz Melodies Using Harmonic Tags </li>
				    <li> Exploiting Music Source Separation for Automatic Lyrics Transcription with Whisper </li>
				    <li> M6(GPT)3: Generating Multitrack Modifiable Multi-Minute MIDI Music from Text using Genetic Algorithms, Probabilistic Methods and GPT Models in any Progression and Time Signature </li>
				    <li> AI Music Artist Toolkit (AIMAT) - A Modular Environment for Experimenting with AI in Music </li>
				  </ol>
				  </p>
				</td>
                            </tr>

                            <tr>
                                <td>03:20PM</td>
                                <td>Break</td>
                            </tr>

                            <tr>
                                <td>03:30PM</td>
                                <td>
				  Panel Discussion <p>
				  Moderator: Gus Xia.
				  Panelists: Geoffroy Peeters, Emmanouil Benetos,  Zhiyao Duan, Ziyu Wang.
                                </td>
                            </tr>

                            <tr>
                                <td>04:30PM</td>
                                <td>Winners of the <a href="https://ai4musicians.org/transcription/2025transcription.html">Transcription Challenge</a>. Moderator: Yung-Hsiang Lu
				   <ol>
				     <li>MIROS (Music Information Retrieval OsnabrÃ¼ck): Deniz GÃ¼n, Fernando Riveros, Paul Koesling, Anish Haluvani Sundresh </li>
				     <li>YourMT3-YPTF-MoE-M:	Sungkyun Chang, Simon Dixon, Emmanouil Benetos </li>
				   </ol>
				</td>
                            </tr>

                            <tr>
                                <td>05:00PM</td>
                                <td>Adjourn</td>
                            </tr>
                        </table>
                    </section>
                    <!-- Schedule -->

                    <!-- Invited Speakers -->
                    <section>
                        <h2>Invited Speakers</h2>

                        <section class="wrapper style1 spotlights">
                            <!-- Speaker 1: Geoffroy Peeters--->
                            <div class="team-member">
                                <a
                                    href="https://perso.telecom-paristech.fr/gpeeters/index.html"
                                    class="image"
                                    target="_blank"
                                    rel="noopener noreferrer"
                                >
                                    <div class="team-photo">
                                        <img
                                            src="https://perso.telecom-paristech.fr/gpeeters/images/photoID.png"
                                            alt="Geoffroy Peeters"
                                            class="profile-img"
                                        />
                                    </div>
                                </a>
                                <div class="team-info">
                                    <h2>
                                        <a
                                            href="https://perso.telecom-paristech.fr/gpeeters/index.html"
                                            target="_blank"
                                            rel="noopener noreferrer"
                                            >Geoffroy Peeters</a
                                        >
                                    </h2>
                                    <p>
                                        Geoffroy Peeters is full-professor in
                                        the (Laboratoire Traitement et
                                        Communication de l'Information ) S2A
                                        team at TÃ©lÃ©com Paris. He received his
                                        PHDs degree in 2001 and Habilitation in
                                        2013 from University Paris-VI on audio
                                        signal processing, data analysis and
                                        machine learning. Before joining TÃ©lÃ©com
                                        Paris, he lead research related to Music
                                        Information Retrieval at IRCAM
                                        ('Institut de recherche et coordination
                                        acoustique/musique). His current
                                        research work is on signal processing,
                                        machine learning and deep learning
                                        applied to audio and music data
                                        analysis.
                                    </p>
                                </div>
                            </div>
 <div class="speaker-talk">
                                    <h3 class="talk-title">
                                 Self-Supervised Learning for Invariant and Equivariant representations
                                    </h3>
                                    <p class="talk-abstract">
                                      <strong>Abstract:</strong>
				      Self-supervised learning aims to apply supervised learning algorithms
				      without the need for annotated data.  It can therefore offer a
				      solution for training ML-based systems in music, a domain where
				      annotated data is often scarce.  In this talk, we review recent
				      advances in self-supervised learning applied to music, focusing on its
				      two main paradigms: invariance (e.g., contrastive, masking,
				      teacher-student, clustering, information-based, multi-modal) and
				      equivariance.  More precisely, we present our contributions: MatPac as
				      foundation models, Stem-JEPA for generation, PESTO for pitch, PESTO-T
				      for tempo, and CPC for beat detection.
                                    </p>
                                </div>
                            </div>
			    
			    <!---
                            <div class="team-member">
                                <a
                                    href="https://csl.sony.fr/member/javier-nistal-hurle/"
                                    class="image"
                                    target="_blank"
                                    rel="noopener noreferrer"
                                >
                                    <div class="team-photo">
                                        <img
                                            src="https://csl.sony.fr/wp-content/uploads/elementor/thumbs/javier-headshot-scaled-puzkwnz7yr9ew134gwqhchl6k4zv9cuus1wob0ycjk.jpg"
                                            alt="Javier Nistal Hurle"
                                            class="profile-img"
                                        />
                                    </div>
                                </a>
                                <div class="team-info">
                                    <h2>
                                        <a
                                            href="https://csl.sony.fr/member/javier-nistal-hurle/"
                                            target="_blank"
                                            rel="noopener noreferrer"
                                            >Javier Nistal Hurle</a
                                        >
                                    </h2>
                                    <p>
                                        Javier Nistal Hurle is an Associate
                                        Researcher with the Music Team at Sony
                                        Computer Science Laboratories in Paris.
                                        He studied Telecommunications
                                        Engineering at Universidad Politecnica
                                        de Madrid and received a Master's in
                                        Sound and Music Computing from
                                        Universitat Pompeu Fabra. He completed
                                        his doctoral studies at Telecom Paris in
                                        a collaborative effort with Sony CSL,
                                        where he researched Generative
                                        Adversarial Networks for musical audio
                                        synthesis. In the music tech industry,
                                        Javier has worked on diverse projects
                                        involving machine learning (ML) and
                                        music, including recommendation systems,
                                        instrument recognition, and automatic
                                        mixing. He contributed to the
                                        development of the Midas Heritage D, the
                                        first ML-driven audio mixing console,
                                        and created DrumGAN, the first
                                        ML-powered sound synthesizer to hit the
                                        market. Javier's current research
                                        interest lies at the intersection of
                                        music production and deep learning. He
                                        is dedicated to devising generative
                                        models for music co-creation, aiming to
                                        enhance artistic creativity and enable
                                        musicians to explore new realms of
                                        musical expression.
                                    </p>
                                </div>
                            </div>
			    --->
                            <!-- Speaker 2: Zhiyao Duan -->
                            <div class="team-member">
                                <a
                                    href="https://www.hajim.rochester.edu/ece/people/faculty/duan_zhiyao/index.html"
                                    class="image"
                                    target="_blank"
                                    rel="noopener noreferrer"
                                >
                                    <div class="team-photo">
                                        <img
                                            src="https://hajim.rochester.edu/ece/sites/zduan/resource/ZhiyaoDuan2018_web.jpg"
                                            alt="Zhiyao Duan"
                                        />
                                    </div>
                                </a>
                                <div class="team-info">
                                    <h2>
                                        <a
                                            href="https://www.hajim.rochester.edu/ece/people/faculty/duan_zhiyao/index.html"
                                            target="_blank"
                                            rel="noopener noreferrer"
                                            >Zhiyao Duan</a
                                        >
                                    </h2>
                                    <p>
                                        Zhiyao Duan is an associate professor in
                                        Electrical and Computer Engineering,
                                        Computer Science, and Data Science at
                                        the University of Rochester. He is also
                                        a co-founder of Violy, a company aiming
                                        to improve music education through AI.
                                        His research interest is in computer
                                        audition and its connections with
                                        computer vision, natural language
                                        processing, and augmented and virtual
                                        reality. He received a best paper award
                                        at the Sound and Music Computing (SMC)
                                        Conference in 2017, a best paper
                                        nomination at the International Society
                                        for Music Information Retrieval (ISMIR)
                                        Conference in 2017, and a CAREER award
                                        from the National Science Foundation
                                        (NSF). His work has been funded by NSF,
                                        National Institute of Health, National
                                        Institute of Justice, New York State
                                        Center of Excellence in Data Science,
                                        and University of Rochester internal
                                        awards on AR/VR, health analytics, and
                                        data science. He is a senior area editor
                                        of IEEE Signal Processing Letters, an
                                        associate editor for IEEE Open Journal
                                        of Signal Processing, and a guest editor
                                        for Transactions of the International
                                        Society for Music Information Retrieval.
                                        He is the President of ISMIR.
                                    </p>
                                </div>
                            </div>

                            <!-- Speaker 3: Fatemeh Jamshidi -->
                            <div class="team-member">
                                <a
                                    href="https://www.cpp.edu/faculty/fjamshidi/index.shtml"
                                    class="image"
                                    target="_blank"
                                    rel="noopener noreferrer"
                                >
                                    <div class="team-photo">
                                        <img
                                            src="https://www.cpp.edu/faculty/fjamshidi/img_5363.jpg"
                                            alt="Fatemeh Jamshidi"
                                        />
                                    </div>
                                </a>
                                <div class="team-info">
                                    <h2>
                                        <a
                                            href="https://www.cpp.edu/faculty/fjamshidi/index.shtml"
                                            target="_blank"
                                            rel="noopener noreferrer"
                                            >Fatemeh Jamshidi</a
                                        >
                                    </h2>
                                    <p>
                                        Fatemeh Jamshidi is an Assistant
                                        Professor in the Department of Computer
                                        Science at Cal Poly Pomona. Her research
                                        spans artificial intelligence, computer
                                        science education, computer music,
                                        machine learning and deep learning in
                                        music, game AI, human-AI collaboration,
                                        as well as augmented and mixed reality.
                                        She has published in prestigious venues,
                                        including ACM SIGCSE, ISMIR, IEEE, and
                                        HCII. Fatemeh earned her Ph.D. in
                                        Computer Science and Software
                                        Engineering and a master's in Music
                                        Education from Auburn University in 2024
                                        and 2023, respectively. During her
                                        Ph.D., she founded the Computing + Music
                                        programs, which have engaged hundreds of
                                        participants from underrepresented
                                        groups since 2018. From 2020 to 2023,
                                        she also served as the Director of the
                                        Persian Music Ensemble at Auburn
                                        University. Her long-term goal is to
                                        establish a music technology center that
                                        fosters undergraduate and graduate
                                        research in areas such as music therapy,
                                        music generation, game music, and mixed
                                        reality in music.
                                    </p>
                                </div>
                            </div>

                            <!-- Speaker 4: Gus Xia -->
                            <div class="team-member">
                                <a
                                    href="https://mbzuai.ac.ae/study/faculty/dr-gus-xia/"
                                    class="image"
                                    target="_blank"
                                    rel="noopener noreferrer"
                                >
                                    <div class="team-photo">
                                        <img
                                          src="https://www.musicxlab.com/members/gus/gus_xia.jpg"
                                            alt="Gus Xia"
                                        />
                                    </div>
                                </a>
                                <div class="team-info">
                                    <h2>
                                        <a
                                            href="https://mbzuai.ac.ae/study/faculty/dr-gus-xia/"
                                            target="_blank"
                                            rel="noopener noreferrer"
                                            >Gus Xia</a
                                        >
                                    </h2>
                                    <p>
                                        Gus Xia is an assistant professor of
                                        Machine Learning at the Mohamed bin
                                        Zayed University of Artificial
                                        Intelligence in Masdar City, Abu Dhabi.
                                        His research includes the design of
                                        interactive intelligent systems to
                                        extend human musical creation and
                                        expression. This research lies at the
                                        intersection of machine learning,
                                        human-computer interaction, robotics,
                                        and computer music. Some representative
                                        works include interactive composition
                                        via style transfer, human-computer
                                        interactive performances, autonomous
                                        dancing robots, large-scale
                                        content-based music retrieval, haptic
                                        guidance for flute tutoring, and
                                        bio-music computing using slime mold.
                                    </p>
                                </div>
                            </div>

                            <!-- Speaker 5: Emmanouil Benetos -->
                            <div class="speaker-section">
                                <div class="speaker-content">
                                    <div class="speaker-photo-container">
                                        <img
                                            src="https://www.turing.ac.uk/sites/default/files/styles/people/public/2018-10/93537270_6947789_678149.jpg?itok=rVJktN49"
                                            alt="Emmanouil Benetos"
                                            class="speaker-photo"
                                        />
                                    </div>

                                    <div class="speaker-bio-container">
                                        <h2 class="speaker-name">
                                            <a
                                                href="https://www.eecs.qmul.ac.uk/~emmanouilb/"
                                                target="_blank"
                                                rel="noopener noreferrer"
                                                >Emmanouil Benetos</a
                                            >
                                        </h2>
                                        <p class="speaker-bio">
                                            Emmanouil Benetos is Reader in
                                            Machine Listening and Director of
                                            Research at the School of Electronic
                                            Engineering and Computer Science of
                                            Queen Mary University of London.
                                            Within Queen Mary, he is member of
                                            the Centre for Digital Music and
                                            Centre for Multimodal AI, is Deputy
                                            Director at the UKRI Centre for
                                            Doctoral Training in AI and Music
                                            (AIM), and co-leads the School's
                                            Machine Listening Lab. His main area
                                            of research is computational audio
                                            analysis, also referred to as
                                            machine listening or computer
                                            audition - with applications to
                                            music, urban, everyday and nature
                                            sounds.
                                        </p>
                                        <p>
                                            Website:
                                            <a
                                                href="https://www.eecs.qmul.ac.uk/~emmanouilb/"
                                                target="_blank"
                                                rel="noopener noreferrer"
                                                >https://www.eecs.qmul.ac.uk/~emmanouilb/</a
                                            >
                                        </p>
                                    </div>
                                </div>

                                <!-- Talk Title and Abstract -->
                                <div class="speaker-talk">
                                    <h3 class="talk-title">
                                        Machine learning paradigms for music and
                                        audio understanding
                                    </h3>
                                    <p class="talk-abstract">
                                        <strong>Abstract:</strong> The area of
                                        computational audio analysis -also
                                        called machine listening- continues to
                                        evolve. Starting from methods grounded
                                        in digital signal processing and
                                        acoustics, followed by supervised
                                        machine learning methods that require
                                        large amounts of labelled data, recent
                                        approaches for learning music audio
                                        representations are fueled by advances
                                        in the broader field of artificial
                                        intelligence. The talk will outline
                                        recent research carried out at the
                                        Centre for Digital Music of Queen Mary
                                        University of London focusing on
                                        emerging learning paradigms for making
                                        sense of music and audio data. Topics
                                        covered will include learning in the
                                        presence of limited audio data, the
                                        inclusion of other modalities such as
                                        natural language to aid learning music
                                        representations, and finally methods for
                                        learning from unlabelled audio data -
                                        with the latter being used as a first
                                        step towards the creation of music
                                        foundation models.
                                    </p>
                                </div>
                            </div>

			    <!-- Panelist: Cyran Aouameur -->
                           

                        </section>
                    </section>

		    <section>
                      <h2>Paper Presentations</h2>
		      <ol>
			<li> Analysis of Improvised Jazz Melodies
			Using Harmonic Tags. Authors: Carey Bunks
			(Queen Mary University of London); Simon
			Dixon (Queen Mary University in London); Bruno
			Di Giorgi (Apple)
			</li>

			<li> Exploiting Music Source Separation for
			Automatic Lyrics Transcription with
			Whisper. Authors: Jaza Syed (Queen Mary
			University of London); Ivan Meresman Higgs
			(Queen Mary University of London); OndÅ™ej
			CÃ­fka (AudioShake); Mark Sandler ( Queen Mary
			University of London)
			</li>
			
			<li> M6(GPT)3: Generating Multitrack
			Modifiable Multi-Minute MIDI Music from Text
			using Genetic Algorithms, Probabilistic
			Methods and GPT Models in any Progression and
			Time Signature. Authors: Jakub PoÄ‡wiardowski
			(Warsaw University of Technology); Mateusz
			Modrzejewski (Warsaw University of
			Technology); Marek S. Tatara (Gdansk
			University of Technology)
			</li>

			
			<li> AI Music Artist Toolkit (AIMAT) - A
			Modular Environment for Experimenting with AI
			  in Music. Authors: Eric Browne (MTU); Michael Clemens (New Jersey Institute of Technology)
			</li>

			
		      </ol>
		    </section>
		    
                    <!-- Organizers -->
                    <section>
                        <h2>Organizers</h2>
                        <p>
                            Meet the team behind the 2025 ICME Workshop on
                            Artificial Intelligence for Music.
                        </p>
                        <section
                            id="organizers"
                            class="wrapper style1 spotlights"
                        >
                            <!-- Organizer 1 -->
                            <div class="team-member">
                                <div class="team-photo">
                                    <img
                                        src="https://engineering.purdue.edu/ResourceDB/ResourceFiles/image277153"
                                        alt="Yung Hsiang Lu"
                                        class="profile-img"
                                    />
                                </div>
                                <div class="team-info">
                                    <h2>
                                        <a
                                            href="https://yhlu.net/"
                                            target="_blank"
                                            rel="noopener noreferrer"
                                            >Yung-Hsiang Lu</a
                                        >
                                    </h2>
                                    <h3>
                                        Professor of Electrical and Computer
                                        Engineering
                                    </h3>
                                    <p>
                                        Yung-Hsiang Lu is a professor in the
                                        Elmore Family School of Electrical and
                                        Computer Engineering at Purdue
                                        University. He is a fellow of the IEEE
                                        and a distinguished scientist of the
                                        ACM. Yung-Hsiang has published papers on
                                        computer vision and machine learning in
                                        venues such as AI Magazine, Nature
                                        Machine Learning, and Computer. He is
                                        one of the editors of the book
                                        "Low-Power Computer Vision: Improve the
                                        Efficiency of Artificial Intelligence"
                                        (ISBN 9780367744700, 2022 by Chapman &
                                        Hall).
                                    </p>
                                </div>
                            </div>

                            <!-- Organizer 2 -->
                            <div class="team-member">
                                <div class="team-photo">
                                    <img
                                        src="images/yun.jpg"
                                        alt="Dr. Kristen Yeon-Ji Yun"
                                        class="profile-img"
                                    />
                                </div>
                                <div class="team-info">
                                    <h2>
                                        <a
                                            href="https://kristenyeonjiyun.com/"
                                            target="_blank"
                                            rel="noopener noreferrer"
                                            >Kristen Yeon-Ji Yun</a
                                        >
                                    </h2>
                                    <h3>
                                        Clinical Associate Professor of Music
                                    </h3>
                                    <p>
                                        Kristen Yeon-Ji Yun is a clinical
                                        associate professor in the Department of
                                        Music at the Patti and Rusty Rueff
                                        School of Design, Art, and Performance
                                        at Purdue University. She is the
                                        Principal Investigator of the research
                                        project "Artificial Intelligence
                                        Technology for Future Music Performers"
                                        (US National Science Foundation, IIS
                                        2326198). Kristen is an active soloist,
                                        chamber musician, musical scholar, and
                                        clinician. She has toured many
                                        countries, including Malaysia, Thailand,
                                        Germany, Mexico, Japan, China, Hong
                                        Kong, Spain, France, Italy, Taiwan, and
                                        South Korea, giving a series of
                                        successful concerts and master classes.
                                    </p>
                                </div>
                            </div>

                            <!-- Organizer 3 -->
                            <div class="team-member">
                                <div class="team-photo">
                                    <img
                                        src="https://www.luc.edu/media/lucedu/computerscience/images/newbwfacultyphotos/George_Thiruvathukal.jpg"
                                        alt="George K. Thiruvathukal"
                                        class="profile-img"
                                    />
                                </div>
                                <div class="team-info">
                                    <h2>
                                        <a
                                            href="https://gkt.sh/"
                                            target="_blank"
                                            rel="noopener noreferrer"
                                            >George K. Thiruvathukal</a
                                        >
                                    </h2>
                                    <h3>
                                        Professor and Chairperson of Computer
                                        Science
                                    </h3>
                                    <p>
                                        George K. Thiruvathukal is a professor
                                        and chairperson of Computer Science at
                                        Loyola University Chicago and a visiting
                                        computer scientist at Argonne National
                                        Laboratory. His research interests
                                        include high-performance computing and
                                        distributed systems, programming
                                        languages, software engineering, machine
                                        learning, digital humanities, and arts
                                        (primarily music). George has published
                                        multiple books, including "Software
                                        Engineering for Science" (ISBN
                                        9780367574277, 2016 Chapman and Hall &
                                        CRC), "Web Programming: Techniques for
                                        Integrating Python, Linux, Apache, and
                                        MySQL" (ISBN 9780130410658, 2001
                                        Prentice Hall), and "High-Performance
                                        Java Platform Computing: Multithreaded
                                        and Networked Programming" (ISBN
                                        9780130161642, 2000 Prentice Hall).
                                    </p>
                                </div>
                            </div>

                            <!-- Add additional organizers following the same structure -->
                        </section>
                    </section>
                    <!-- Technical Program Committee -->
                    <section>
                        <h2>Technical Program Committee</h2>
                        <section class="wrapper style1 spotlights">
                            <!-- TPC 1 -->
                            <div class="team-member">
                                <div class="team-photo">
                                    <img
                                        src="https://www.qmul.ac.uk/eecs/media/eecs/staff-profile-images/67634.jpg"
                                        alt="Charalampos Saitis"
                                        class="profile-img"
                                    />
                                </div>
                                <div class="team-info">
                                    <h2>
                                        <a
                                            href="https://www.qmul.ac.uk/eecs/people/profiles/saitischaralampos.html"
                                            target="_blank"
                                            rel="noopener noreferrer"
                                            >Charalampos Saitis</a
                                        >
                                    </h2>
                                    <h3>
                                        Lecturer in Digital Music Processing
                                    </h3>
                                    <p>
                                        Dr. Saitis is an assistant professor in
                                        digital music processing at Queen Mary
                                        University of London where he leads the
                                        Communication Acoustics Lab (COMMA) at
                                        the Centre for Digital Music (C4DM) and
                                        is a co-investigator in the UKRI CDT in
                                        AI and Music based (2019-2028).
                                        Experienced leader in the intersecting
                                        fields of cognitive science, music
                                        informatics and generative AI with
                                        applications in sonic creativity,
                                        recommender systems and well-being.
                                    </p>
                                </div>
                            </div>

                            <!-- TPC 2 -->
                            <div class="team-member">
                                <a
                                    href="https://hermandong.com/"
                                    target="_blank"
                                    rel="noopener noreferrer"
                                >
                                    <div class="team-photo">
                                        <img
                                            src="https://hermandong.com/headshot.jpg"
                                            alt="Hao-Wen (Herman) Dong"
                                            class="profile-img"
                                        />
                                    </div>
                                </a>
                                <div class="team-info">
                                    <h2>
                                        <a
                                            href="https://hermandong.com/"
                                            target="_blank"
                                            rel="noopener noreferrer"
                                            >Hao-Wen (Herman) Dong</a
                                        >
                                    </h2>
                                    <p>
                                        Hao-Wen (Herman) Dong is an Assistant
                                        Professor in the Performing Arts
                                        Technology Department at the University
                                        of Michigan. Herman's research aims to
                                        empower music and audio creation with
                                        machine learning. His long-term goal is
                                        to lower the barrier of entry for music
                                        composition and democratize audio
                                        content creation. He is broadly
                                        interested in music generation, audio
                                        synthesis, multimodal machine learning,
                                        and music information retrieval. Herman
                                        received his PhD degree in Computer
                                        Science from the University of
                                        California San Diego, where he worked
                                        with Julian McAuley and Taylor
                                        Berg-Kirkpatrick. His research has been
                                        recognized by the UCSD CSE Doctoral
                                        Award for Excellence in Research, KAUST
                                        Rising Stars in AI, UChicago and UCSD
                                        Rising Stars in Data Science, ICASSP
                                        Rising Stars in Signal Processing, and
                                        UCSD GPSA Interdisciplinary Research
                                        Award.
                                    </p>
                                </div>
                            </div>

                            <!-- TPC 3 -->
                            <div class="team-member">
                                <div class="team-photo">
                                    <img
                                        src="https://sse.umkc.edu/profiles/profile-pictures/mei-ling_shyu_ece.jpg"
                                        alt="Mei-Ling Shyu"
                                        class="profile-img"
                                    />
                                </div>
                                <div class="team-info">
                                    <h2>
                                        <a
                                            href="https://sse.umkc.edu/profiles/shyu-mei-ling.html"
                                            target="_blank"
                                            rel="noopener noreferrer"
                                            >Mei-Ling Shyu</a
                                        >
                                    </h2>
                                    <h3>Professor Science and Engineering</h3>
                                    <p>
                                        Dr. Shyu is a professor of Electrical
                                        and Computer Engineering, at the
                                        University of Missouri-Kansas City.
                                        Prior to UMKC, she was the Associate
                                        Chair and Professor at the Department of
                                        Electrical and Computer Engineering at
                                        the University of Miami. She received
                                        her PhD degree from the School of
                                        Electrical and Computer Engineering and
                                        three Master's degrees in Computer
                                        Science, Electrical Engineering, and
                                        Restaurant, Hotel, Institutional, and
                                        Tourism Management, all from Purdue
                                        University. Her research interests
                                        include data science, AI, machine
                                        learning, data mining, big data
                                        analytics, multimedia information
                                        systems, and semantic-based information
                                        management/fusion/retrieval.
                                    </p>
                                </div>
                            </div>

                            <!-- TPC 4 -->
                            <div class="team-member">
                                <div class="team-photo">
                                    <img
                                        src="https://www.csie.ntu.edu.tw/~wenhuang/images/whcheng.jpg"
                                        alt="Wen-Huang Cheng"
                                        class="profile-img"
                                    />
                                </div>
                                <div class="team-info">
                                    <h2>
                                        <a
                                            href="https://www.csie.ntu.edu.tw/~wenhuang/"
                                            target="_blank"
                                            rel="noopener noreferrer"
                                            >Wen-Huang Cheng</a
                                        >
                                    </h2>
                                    <h3>
                                        Distinguished Chair Professor Department
                                        of Computer Science and Information
                                        Engineering
                                    </h3>
                                    <p>
                                        Dr. Cheng is a professor of Computer
                                        Science and Information Engineering at
                                        National Taiwan University. He is the
                                        founding director of the Artificial
                                        Intelligence and Multimedia (AIMM)
                                        Research Group. Before joining National
                                        Taiwan University, he was a
                                        Distinguished Professor at the Institute
                                        of Electronics, National Yang Ming Chiao
                                        Tung University, and led the Multimedia
                                        Computing Research Group at the Research
                                        Center for Information Technology
                                        Innovation at Academia Sinica. He is a
                                        fellow of the IEEE and Asia-Pacific
                                        Artificial Intelligence Association.
                                    </p>
                                </div>
                            </div>
                        </section>
                    </section>
                </div>
            </section>
        </div>

        <!-- Scripts -->
        <script src="assets/js/jquery.min.js"></script>
        <script src="assets/js/jquery.scrollex.min.js"></script>
        <script src="assets/js/jquery.scrolly.min.js"></script>
        <script src="assets/js/browser.min.js"></script>
        <script src="assets/js/breakpoints.min.js"></script>
        <script src="assets/js/util.js"></script>
        <script src="assets/js/main.js"></script>
    </body>
</html>
