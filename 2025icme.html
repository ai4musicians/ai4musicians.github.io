<!DOCTYPE html>
<html lang="en" xml:lang="en">
  <head>
    <title>2025 ICME Workshop AI for Music</title>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, user-scalable=no"
      />
    <meta
      name="description"
      content="Join the 2025 ICME Workshop on Artificial Intelligence for Music, exploring the intersection of AI and music creation, recognition, education, and more."
      />
    <link rel="stylesheet" href="assets/css/main.css" />
    <noscript>
      <link rel="stylesheet" href="assets/css/noscript.css" />
    </noscript>
  </head>

  <body class="is-preload">
    <!-- Sidebar -->
    <section id="sidebar">
      <div class="inner">
	<nav>
	  <ul>
	    <li>
	      <a href="index.html"
		 ><img src="images/aim.png" width="50" alt="AIM Logo"
		       /></a>
	    </li>
	    <li><a href="index.html#news">News</a></li>
	    <li><a href="index.html#amt">Our Projects</a></li>
	    <li><a href="index.html#activities">Research Areas</a></li>
	    <li><a href="index.html#who">Leaders</a></li>
	    <li><a href="index.html#outreach">Outreach</a></li>
	    <li><a href="index.html#vipteam">VIP Team</a></li>
	    <li><a href="index.html#contact">Contact</a></li>
	    <li><a href="#call-for-papers">Call for Papers</a></li>
	    <!-- Added Navigation Link -->
	  </ul>
	</nav>
      </div>
    </section>

    <!-- Wrapper -->
    <div id="wrapper">
      <!-- Main -->
      <section id="main" class="wrapper style1 fullscreen fade-up">
	<div class="inner">
	  <h1 class="major">Artificial Intelligence for Music</h1>

	  A workshop at
	  <a
	    href="https://2025.ieeeicme.org/"
	    target="_blank"
	    rel="noopener noreferrer"
	    >2025 ICME Annual Conference</a
					  >

	  <!-- Summary -->

	  <section>
	    <h2>Date: TBD (between 2025/06/30 - 2025/07/04) </h2>
	    <h2>Workshop Summary</h2>
	    <p>

	      Music is an essential component of multimedia
	      content. This workshop will explore the dynamic
	      intersection of artificial intelligence and music. This
	      workshop investigates how AI is changing the music
	      industry and education, from composition to performance,
	      production, collaboration, and audience
	      experience. Participants will gain insights into the
	      ways AI can enhance creativity and enable musicians and
	      producers to push the boundaries of their art. The
	      workshop will also discuss AI's impacts on music
	      education and the careers of musicians. We will cover
	      topics such as AI-driven music composition, where
	      algorithms generate melodies, harmonies, and even full
	      orchestral arrangements. Computer-generated music may be
	      combined with computer-generated video to create the
	      entire multimedia content. The workshop will discuss how
	      AI tools can assist in sound design, remixing, and
	      mastering, allowing for new sonic possibilities and
	      efficiencies in music production. Additionally, the
	      workshop will discuss the legal and ethical implications
	      of AI in music, including questions of authorship,
	      originality, and the role of the human artist in an
	      increasingly automated world. This workshop is designed
	      for AI researchers, musicians, producers, and educators
	      interested in the status and future of AI in music.

	      The organizing team will hold a competition for
	      Automatic Music Transcription (AMT). This online competition will
	      accept submissions worldwide, including both academia and
	      industry. The winners will present their solutions at this ICME
	      workshop. This competition is sponsored by the IEEE Technical
	      Community on Multimedia Computing (TCMC) and the Computer
	      Society. More details about this challenge will be
	      available <a href="https://ai4musicians.org/transcription/2025transcription.html">here</a>.
	    </p>
	  </section>

	  <!-- Call for Papers -->
	  <section id="call-for-papers">
	    <div class="inner">
	      <h2>Call for Papers</h2>

	      This one-day workshop will explore the dynamic
	      intersection of artificial intelligence and multimedia
	      with an emphasis on music and audio technologies. The
	      workshop explores how AI is transforming music creation,
	      recognition, and education, ethical and legal
	      implications, as well as business opportunities. We will
	      investigate how AI is changing the music industry and
	      education—from composition to performance, production,
	      collaboration, and audience experience. Participants
	      will gain insights into the technological challenges in
	      music and how AI can enhance creativity, enabling
	      musicians and producers to push the boundaries of their
	      art. The workshop will cover topics such as AI-driven
	      music composition, where algorithms generate melodies,
	      harmonies, and even full orchestral arrangements. We
	      will discuss how AI tools assist in sound design,
	      remixing, and mastering, allowing for new sonic
	      possibilities and efficiencies in music
	      production. Additionally, we'll examine AI's impact on
	      music education and the careers of musicians, exploring
	      advanced learning tools and teaching methods. AI
	      technologies are increasingly adopted in the music and
	      entertainment industry. The workshop will also discuss
	      the legal and ethical implications of AI in music,
	      including questions of authorship, originality, and the
	      evolving role of human artists in an increasingly
	      automated world. This workshop is designed for AI
	      researchers, musicians, producers, and educators
	      interested in the current status and future of AI in
	      music.
	      
	      <h3>Topics of Interest</h3>

	      Topics of Interest include, but are not limited to
	      
	      <ul>
		<li>AI-Driven Music Composition and Generation</li>
		<li>AI in Music Practice and Performance</li>
		<li>AI-based Music Recognition and Transcription</li>
		<li>AI Applications in Sound Design</li>
		<li>AI-Generated Videos to Accompany Music</li>
		<li>AI-Generated Lyrics Based on Music</li>
		<li>Legal or Ethical Implications of AI on Music</li>
		<li>AI's Impacts on Musicians’ Careers</li>
		<li>AI Assisted Music Education</li>
		<li>Business Opportunities of AI and Music</li>
		<li>Music Datasets and Data Analysis</li>
	      </ul>

	      <h3>Submission Requirements</h3>
	      <p>

		Please follow the submission requirements
		of <a href="https://2025.ieeeicme.org/author-information-and-submission-instructions/">ICME
		2025</a>. Papers must be no longer than 6 pages,
		including all text, figures, and references.  This
		workshop will follow ICME submission and
		adopt <i>double blind</i> reviews. Authors should not
		identify themselves in the submitted PDF files.
		
		<p>
		Work in progress is welcome. Authors are encouraged to include
		descriptions of their prototype implementations. Additionally,
		authors are encouraged to interact with workshop attendees by
		including posters or demonstrations at the end of the workshop.
		Conceptual designs without any evidence of practical
		implementation are discouraged.
		</p>

		Submit papers to <a href="https://cmt3.research.microsoft.com/docs/help/general/request-conference.html#website-requirements">CMT</a>.


	      <h3>Important Dates</h3>
	      <ul>
		<li><strong>Submission Deadline:</strong> March 25, 2025</li>
		<li>
		  <strong>Notification of Acceptance:</strong> April 20, 2025
		</li>
		<li><strong>Final Version Due:</strong> May 10, 2025</li>
	      </ul>

	      <p>Accepted papers will be posted on the workshop website.</p>
	    </div>
	  </section>


	  <!-- Schedule -->
	  <section>
	    <h2>Workshop Schedule</h2>

	    <table class="modern-table">
	      <tr>
		<th>Time</th>
		<th>Topic</th>
	      </tr>
	      <tr>
		<td>09:00AM</td>
		<td>Welcome by Organizers</td>
	      </tr>
	      <tr>
		<td>09:10AM</td>
		<td>Invited Speech</td>
	      </tr>

	      <tr>
		<td>09:50AM</td>
		<td>Invited Speech</td>
	      </tr>
	      <tr>
		<td>10:30AM</td>
		<td>Break</td>
	      </tr>

	      <tr>
		<td>11:00AM</td>
		<td>Paper Presentations (selected from submissions)</td>
	      </tr>

	      <tr>
		<td>12:00PM</td>
		<td>Lunch Break</td>
	      </tr>

	      <tr>
		<td>01:00PM</td>
		<td>Invited Speech</td>
	      </tr>

	      <tr>
		<td>01:40PM</td>
		<td>Invited Speech</td>
	      </tr>

	      <tr>
		<td>02:20PM</td>
		<td>Panel Discussion by the Invited Speakers</td>
	      </tr>

	      <tr>
		<td>03:20PM</td>
		<td>Break</td>
	      </tr>

	      <tr>
		<td>03:30PM</td>
		<td>Paper Presentations (selected from submissions)</td>
	      </tr>

	      <tr>
		<td>04:30PM</td>
		<td>Open Discussion: Future of AI and Music</td>
	      </tr>

	      <tr>
		<td>05:00PM</td>
		<td>Adjourn</td>
	      </tr>
	    </table>
	  </section>
	  <!-- Schedule -->

	  <!-- Invited Speakers -->
	  <section>
	    <h2>Invited Speakers</h2>

	    <!-- Speaker 1: Javier Nistal Hurle--->
	    <section class="wrapper style1 spotlights">
	      <section>
		<a
		  href="https://csl.sony.fr/member/javier-nistal-hurle/"
		  class="image"
		  target="_blank"
		  rel="noopener noreferrer"
		  >
		  <img
		    src="https://csl.sony.fr/wp-content/uploads/elementor/thumbs/javier-headshot-scaled-puzkwnz7yr9ew134gwqhchl6k4zv9cuus1wob0ycjk.jpg"
		    width="400"
		    alt="Javier Nistal Hurle"
		    data-position="top center"
		    />
		</a>
		<div class="content">
		  <div class="inner">
		    <h2>
		      <a
			href="https://csl.sony.fr/member/javier-nistal-hurle/"
			target="_blank"
			rel="noopener noreferrer">Javier Nistal Hurle</a>
		    </h2>
		    <p>
		      Javier Nistal Hurle is an Associate Researcher
		      with the Music Team at Sony Computer Science
		      Laboratories in Paris. He studied
		      Telecommunications Engineering at Universidad
		      Politecnica de Madrid and received a Master’s in
		      Sound and Music Computing from Universitat
		      Pompeu Fabra. He completed his doctoral studies
		      at Telecom Paris in a collaborative effort with
		      Sony CSL, where he researched Generative
		      Adversarial Networks for musical audio
		      synthesis. In the music tech industry, Javier
		      has worked on diverse projects involving machine
		      learning (ML) and music, including
		      recommendation systems, instrument recognition,
		      and automatic mixing. He contributed to the
		      development of the Midas Heritage D, the first
		      ML-driven audio mixing console, and created
		      DrumGAN, the first ML-powered sound synthesizer
		      to hit the market. Javier’s current research
		      interest lies at the intersection of music
		      production and deep learning. He is dedicated to
		      devising generative models for music
		      co-creation, aiming to enhance artistic
		      creativity and enable musicians to explore new
		      realms of musical expression.
		    </p>
		  </div>
		</div>
	      </section>

	      <!-- Speaker 2: Zhiyao Duan -->
	      <section>
		<a
		  href="https://www.hajim.rochester.edu/ece/people/faculty/duan_zhiyao/index.html"
		  class="image"
		  target="_blank"
		  rel="noopener noreferrer"
		  >
		  <img
		    src="https://hajim.rochester.edu/ece/sites/zduan/resource/ZhiyaoDuan2018_web.jpg"
		    width="400"
		    alt="Zhiyao Duan"
		    data-position="top center"
		    />
		</a>
		<div class="content">
		  <div class="inner">
		    <h2>
		      <a
			href="https://www.hajim.rochester.edu/ece/people/faculty/duan_zhiyao/index.html"
			target="_blank"
			rel="noopener noreferrer">Zhiyao Duan</a>
		    </h2>
		    <p>
		      Zhiyao Duan is an associate professor in Electrical and
		      Computer Engineering, Computer Science, and Data Science
		      at the University of Rochester. He is also a co-founder of
		      Violy, a company aiming to improve music education through
		      AI. His research interest is in computer audition and its
		      connections with computer vision, natural language
		      processing, and augmented and virtual reality. He received
		      a best paper award at the Sound and Music Computing (SMC)
		      Conference in 2017, a best paper nomination at the
		      International Society for Music Information Retrieval
		      (ISMIR) Conference in 2017, and a CAREER award from the
		      National Science Foundation (NSF). His work has been
		      funded by NSF, National Institute of Health, National
		      Institute of Justice, New York State Center of Excellence
		      in Data Science, and University of Rochester internal
		      awards on AR/VR, health analytics, and data science. He is
		      a senior area editor of IEEE Signal Processing Letters, an
		      associate editor for IEEE Open Journal of Signal
		      Processing, and a guest editor for Transactions of the
		      International Society for Music Information Retrieval. He
		      is the President of ISMIR.
		    </p>
		  </div>
		</div>
	      </section>

	      <!-- Speaker 3: Fatemeh Jamshidi -->
	      <section>
		<a
		  href="https://www.cpp.edu/faculty/fjamshidi/index.shtml"
		  class="image"
		  target="_blank"
		  rel="noopener noreferrer"
		  >
		  <img
		    src="https://www.cpp.edu/faculty/fjamshidi/img_5363.jpg"
		    width="400"
		    alt="Fatemeh Jamshidi"
		    data-position="top center"/>
		</a>
		<div class="content">
		  <div class="inner">
		    <h2>
		      <a
			href="https://www.cpp.edu/faculty/fjamshidi/index.shtml"
			target="_blank"
			rel="noopener noreferrer">Fatemeh Jamshidi</a>
		    </h2>
		    <p>
		      Fatemeh Jamshidi is an Assistant Professor in
		      the Department of Computer Science at Cal Poly
		      Pomona. Her research spans artificial
		      intelligence, computer science education,
		      computer music, machine learning and deep
		      learning in music, game AI, human-AI
		      collaboration, as well as augmented and mixed
		      reality. She has published in prestigious
		      venues, including ACM SIGCSE, ISMIR, IEEE, and
		      HCII. Fatemeh earned her Ph.D. in Computer
		      Science and Software Engineering and a master’s
		      in Music Education from Auburn University in
		      2024 and 2023, respectively. During her Ph.D.,
		      she founded the Computing + Music programs,
		      which have engaged hundreds of participants from
		      underrepresented groups since 2018. From 2020 to
		      2023, she also served as the Director of the
		      Persian Music Ensemble at Auburn University. Her
		      long-term goal is to establish a music
		      technology center that fosters undergraduate and
		      graduate research in areas such as music
		      therapy, music generation, game music, and mixed
		      reality in music.
		    </p>
		  </div>
		</div>
	      </section>

	      <!-- Speaker 4: Gus Xia -->
	      <section>
		<a
		  href="https://mbzuai.ac.ae/study/faculty/dr-gus-xia/"
		  class="image"
		  target="_blank"
		  rel="noopener noreferrer"
		  >
		  <img
		    src="https://mbzuai.ac.ae/wp-content/uploads/2022/06/profile_gus-xia_secondary.jpg"
		    width="400"
		    alt="Gus Xia"
		    data-position="top center"
		    />
		</a>
		<div class="content">
		  <div class="inner">
		    <h2>
		      <a
			href="https://mbzuai.ac.ae/study/faculty/dr-gus-xia/"
			target="_blank"
			rel="noopener noreferrer"
			>Gus Xia</a
				  >
		    </h2>
		    <p>
		      Gus Xia is an assistant professor of Machine Learning at
		      the Mohamed bin Zayed University of Artificial
		      Intelligence in Masdar City, Abu Dhabi. His research
		      includes the design of interactive intelligent systems to
		      extend human musical creation and expression. This
		      research lies at the intersection of machine learning,
		      human-computer interaction, robotics, and computer music.
		      Some representative works include interactive composition
		      via style transfer, human-computer interactive
		      performances, autonomous dancing robots, large-scale
		      content-based music retrieval, haptic guidance for flute
		      tutoring, and bio-music computing using slime mold.
		    </p>
		  </div>
		</div>
	      </section>
	    </section>
	  </section>

	  <!-- Organizers -->
	  <section>
	    <h2>Organizers</h2>
	    <p>
	      Meet the team behind the 2025 ICME Workshop on Artificial
	      Intelligence for Music.
	    </p>
	    <section id="organizers" class="wrapper style1 spotlights">
	      <!-- Organizer 1 -->
	      <section>
		<a href="#" class="image"
		   ><img
		      src="https://engineering.purdue.edu/ResourceDB/ResourceFiles/image277153"
		      width="400"
		      alt="Yung Hsiang Lu"
		      data-position="top
				     center"
		      /></a>
		<div class="content">
		  <div class="inner">
		    <h2>
		      <a
			href="https://yhlu.net/"
			target="_blank"
			rel="noopener noreferrer"
			>Yung-Hsiang Lu</a
					 >
		    </h2>
		    <h3>Professor of Electrical and Computer Engineering</h3>
		    <p>
		      Yung-Hsiang Lu is a professor in the Elmore Family School
		      of Electrical and Computer Engineering at Purdue
		      University. He is a fellow of the IEEE and a distinguished
		      scientist of the ACM. Yung-Hsiang has published papers on
		      computer vision and machine learning in venues such as AI
		      Magazine, Nature Machine Learning, and Computer. He is one
		      of the editors of the book "Low-Power Computer Vision:
		      Improve the Efficiency of Artificial Intelligence" (ISBN
		      9780367744700, 2022 by Chapman & Hall).
		    </p>
		  </div>
		</div>
	      </section>

	      <!-- Organizer 2 -->
	      <section>
		<a href="#" class="image"
		   ><img
		      src="images/yun.jpg"
		      width="400"
		      alt="Kristen Yeon-Ji Yun"
		      data-position="top center"
		      /></a>
		<div class="content">
		  <div class="inner">
		    <h2>
		      <a
			href="https://kristenyeonjiyun.com/"
			target="_blank"
			rel="noopener noreferrer"
			>Kristen Yeon-Ji Yun</a
					      >
		    </h2>
		    <h3>Clinical Associate Professor of Music</h3>
		    <p>
		      Kristen Yeon-Ji Yun is a clinical associate professor in
		      the Department of Music at the Patti and Rusty Rueff
		      School of Design, Art, and Performance at Purdue
		      University. She is the Principal Investigator of the
		      research project "Artificial Intelligence Technology for
		      Future Music Performers" (US National Science Foundation,
		      IIS 2326198). Kristen is an active soloist, chamber
		      musician, musical scholar, and clinician. She has toured
		      many countries, including Malaysia, Thailand, Germany,
		      Mexico, Japan, China, Hong Kong, Spain, France, Italy,
		      Taiwan, and South Korea, giving a series of successful
		      concerts and master classes.
		    </p>
		  </div>
		</div>
	      </section>

	      <!-- Organizer 3 -->
	      <section>
		<a href="#" class="image"
		   ><img
		      src="https://gkt.sh/content/images/size/w1140/2024/06/George_Thiruvathukal-2-1.jpg"
		      width="400"
		      alt="George K. Thiruvathukal"
		      data-position="top center"
		      /></a>

		<div class="content">
		  <div class="inner">
		    <h2>
		      <a
			href="https://gkt.sh/"
			target="_blank"
			rel="noopener noreferrer"
			>George K. Thiruvathukal</a
						  >
		    </h2>
		    <h3>Professor and Chairperson of Computer Science</h3>
		    <p>
		      George K. Thiruvathukal is a professor and chairperson of
		      Computer Science at Loyola University Chicago and a
		      visiting computer scientist at Argonne National
		      Laboratory. His research interests include
		      high-performance computing and distributed systems,
		      programming languages, software engineering, machine
		      learning, digital humanities, and arts (primarily music).
		      George has published multiple books, including "Software
		      Engineering for Science" (ISBN 9780367574277, 2016 Chapman
		      and Hall & CRC), "Web Programming: Techniques for
		      Integrating Python, Linux, Apache, and MySQL" (ISBN
		      9780130410658, 2001 Prentice Hall), and "High-Performance
		      Java Platform Computing: Multithreaded and Networked
		      Programming" (ISBN 9780130161642, 2000 Prentice Hall).
		    </p>
		  </div>
		</div>
	      </section>

	      <!-- Organizer 4 -->
	      <section>
		<a href="#" class="image"
		   ><img
		      src="images/benchou.jpg"
		      width="400"
		      alt="Benjamin Shiue-Hal Chou"
		      data-position="right center"/></a>
		<div class="content">
		  <div class="inner">
		    <h2>
		      <a
			href="https://www.linkedin.com/in/benjamin-chou-6aa058228/"
			target="_blank"
			rel="noopener noreferrer"
			>Benjamin Shiue-Hal Chou</a
						  >
		    </h2>
		    <h3>PhD Student and Lab Graduate Mentor</h3>
		    <p>
		      Benjamin Shiue-Hal Chou is a PhD student in Electrical and
		      Computer Engineering at Purdue University, supervised by
		      Dr. Yung-Hsiang Lu. His research focuses on artificial
		      intelligence applications in music technology,
		      particularly on detecting errors in music performances.
		      Benjamin has co-authored “Token Turing Machines are
		      Efficient Vision Models” (arXiv preprint arXiv:2409.07613,
		      2024). He earned his Bachelor of Science in Electrical
		      Engineering from National Cheng Kung University (NCKU) in
		      Taiwan, receiving awards such as the Outstanding Student
		      Scholarship, Transnational Research Scholarship Grant, and
		      the Tainan City Digital Governance Talent Award.
		    </p>
		  </div>
		</div>
	      </section>

	      <!-- Add additional organizers following the same structure -->
	    </section>
	  </section>
	  <!-- Technical Program Committee -->
	  <section>
	    <h2>Technical Program Committee</h2>
	    
	    <!-- TPC 1 -->

	      <section>
		<a href="#" class="image"
		   ><img
		      src="https://www.qmul.ac.uk/eecs/media/eecs/staff-profile-images/67634.jpg"
		      width="400"
		      alt="Charalampos Saitis"
		      data-position="right center"/></a>
		<div class="content">
		  <div class="inner">
		    <h2>
		      <a
			href="https://www.qmul.ac.uk/eecs/people/profiles/saitischaralampos.html"
			target="_blank"
			rel="noopener noreferrer">Charalampos Saitis</a>
						  
		    </h2>
		    <h3>Lecturer in Digital Music Processing</h3>
		    <p>
		      Dr. Saitis is an assistant professor in digital
		      music processing at Queen Mary University of London where he leads the
		      Communication Acoustics Lab (COMMA) at the Centre for Digital Music
		      (C4DM) and is a co-investigator in the UKRI CDT in AI and Music based
		      (2019–2028).  Experienced leader in the intersecting fields of
		      cognitive science, music informatics and generative AI with
		      applications in sonic creativity, recommender systems and well-being.
		    </p>
		  </div>
		</div>
	      </section>
	      <!-- TPC 2 -->

	    
	    <section>
		<a
		  href="https://hermandong.com/"
		  class="image"
		  target="_blank"
		  rel="noopener noreferrer"
		  >
		  <img
		    src="https://hermandong.com/profile.jpg"
		    width="400"
		    alt="Hao-Wen (Herman) Dong"
		    data-position="top center"
		    />
		</a>
		<div class="content">
		  <div class="inner">
		    <h2>
		      <a
			href="https://hermandong.com/"
			target="_blank"
			rel="noopener noreferrer"
			>Hao-Wen (Herman) Dong</a
						>
		    </h2>
		    <p>
		      Hao-Wen (Herman) Dong is an Assistant Professor in the
		      Performing Arts Technology Department at the University of
		      Michigan. Herman’s research aims to empower music and
		      audio creation with machine learning. His long-term goal
		      is to lower the barrier of entry for music composition and
		      democratize audio content creation. He is broadly
		      interested in music generation, audio synthesis,
		      multimodal machine learning, and music information
		      retrieval. Herman received his PhD degree in Computer
		      Science from the University of California San Diego, where
		      he worked with Julian McAuley and Taylor Berg-Kirkpatrick.
		      His research has been recognized by the UCSD CSE Doctoral
		      Award for Excellence in Research, KAUST Rising Stars in
		      AI, UChicago and UCSD Rising Stars in Data Science, ICASSP
		      Rising Stars in Signal Processing, and UCSD GPSA
		      Interdisciplinary Research Award.
		    </p>
		  </div>
		</div>
	    </section>
	      
	      <!-- TPC 3 -->
	      <section>
		<a href="#" class="image"
		   ><img
		      src="https://sse.umkc.edu/profiles/profile-pictures/mei-ling_shyu_ece.jpg"
		      width="400"
		      alt="Mei-Ling Shyu"
		      data-position="right center"/></a>
		<div class="content">
		  <div class="inner">
		    <h2>
		      <a
			href="https://sse.umkc.edu/profiles/shyu-mei-ling.html"
			target="_blank"
			rel="noopener noreferrer">Mei-Ling Shyu</a>
		    </h2>
		    <h3>Professor Science and Engineering</h3>
		    <p>
		      Dr. Shyu is a professor of Electrical and
		      Computer Engineering, at the University of
		      Missouri-Kansas City. Prior to UMKC, she was the
		      Associate Chair and Professor at the Department
		      of Electrical and Computer Engineering at the
		      University of Miami. She received her PhD degree
		      from the School of Electrical and Computer
		      Engineering and three Master's degrees in
		      Computer Science, Electrical Engineering, and
		      Restaurant, Hotel, Institutional, and Tourism
		      Management, all from Purdue University. Her
		      research interests include data science, AI,
		      machine learning, data mining, big data
		      analytics, multimedia information systems, and
		      semantic-based information
		      management/fusion/retrieval.
		    </p>
		  </div>
		</div>
	      </section>
	      <!-- Organizer 4 -->
	      <section>
		<a href="#" class="image"
		   ><img
		      src="https://www.csie.ntu.edu.tw/~wenhuang/images/whcheng.jpg"
		      width="400"
		      alt="Wen-Huang Cheng"
		      data-position="right center"/></a>
		<div class="content">
		  <div class="inner">
		    <h2>
		      <a
			href="https://www.csie.ntu.edu.tw/~wenhuang/"
			target="_blank"
			rel="noopener noreferrer">Wen-Huang Cheng</a>
		    </h2>
		    <h3>Distinguished Chair Professor Department of Computer Science and Information Engineering</h3>
		    <p>

		      Dr. Cheng is a professor of Computer Science and
		      Information Engineering at National Taiwan
		      University. He is the founding director of the
		      Artificial Intelligence and Multimedia (AIMM)
		      Research Group. Before joining National Taiwan
		      University, he was a Distinguished Professor at
		      the Institute of Electronics, National Yang Ming
		      Chiao Tung University, and led the Multimedia
		      Computing Research Group at the Research Center
		      for Information Technology Innovation at
		      Academia Sinica. He is a fellow of the IEEE and
		      Asia-Pacific Artificial Intelligence
		      Association.
		    </p>
		  </div>
		</div>
	      </section>
	  </section>
	</div>
      </section>
    </div>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.scrollex.min.js"></script>
    <script src="assets/js/jquery.scrolly.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>
  </body>
</html>
