<!DOCTYPE html>
<html lang="en" xml:lang="en">

<head>
	<title>2025 AAAI Workshop AI for Music</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<meta name="description"
		content="Join the 2025 AAAI Workshop on Artificial Intelligence for Music, exploring the intersection of AI and music creation, recognition, education, and more." />
	<link rel="stylesheet" href="assets/css/main.css" />
	<noscript>
		<link rel="stylesheet" href="assets/css/noscript.css" />
	</noscript>
</head>

<body class="is-preload">
	<!-- Sidebar -->
	<section id="sidebar">
		<div class="inner">
			<nav>
				<ul>
					<li>
						<a href="index.html"><img src="images/aim.png" width="50" alt="AIM Logo" /></a>
					</li>
					<li><a href="index.html#news">News</a></li>
					<li><a href="index.html#amt">Our Projects</a></li>
					<li><a href="index.html#activities">Research Areas</a></li>
					<li><a href="index.html#who">Leaders</a></li>
					<li><a href="index.html#outreach">Outreach</a></li>
					<li><a href="index.html#vipteam">VIP Team</a></li>
					<li><a href="index.html#contact">Contact</a></li>
					<li><a href="#call-for-papers">Call for Papers</a></li>
					<!-- Added Navigation Link -->
				</ul>
			</nav>
		</div>
	</section>

	<!-- Wrapper -->
	<div id="wrapper">
		<!-- Main -->
		<section id="main" class="wrapper style1 fullscreen fade-up">
			<div class="inner">
				<h1 class="major">Artificial Intelligence for Music</h1>

				<p class="subtitle">
					A workshop at
					<a href="https://aaai.org/conference/aaai/aaai-25/" target="_blank" rel="noopener noreferrer">
						2025 AAAI Annual Conference
					</a>
				</p>

				<!-- Summary -->

				<section>
					<h2>2025/03/03 Monday</h2>
					<h2>Workshop Summary</h2>
					<p>
						This one-day workshop will explore the dynamic intersection of
						artificial intelligence and music. It explores how AI is
						transforming music creation, recognition, and education, ethical
						and legal implications, as well as business opportunities. We will
						investigate how AI is changing the music industry and
						education—from composition to performance, production,
						collaboration, and audience experience. Participants will gain
						insights into the technological challenges in music and how AI can
						enhance creativity, enabling musicians and producers to push the
						boundaries of their art. The workshop will cover topics such as
						AI-driven music composition, where algorithms generate melodies,
						harmonies, and even full orchestral arrangements. We will discuss
						how AI tools assist in sound design, remixing, and mastering,
						allowing for new sonic possibilities and efficiencies in music
						production. Additionally, we'll examine AI's impact on music
						education and the careers of musicians, exploring advanced
						learning tools and teaching methods. AI technologies are
						increasingly adopted in the music and entertainment industry. The
						workshop will also discuss the legal and ethical implications of
						AI in music, including questions of authorship, originality, and
						the evolving role of human artists in an increasingly automated
						world. This workshop is designed for AI researchers, musicians,
						producers, and educators interested in the current status and
						future of AI in music.
					</p>
				</section>

				<section>
					<h2>Topics</h2>
					<ul>
						<li>
							Impacts of AI on music education and careers of musicians.
						</li>
						<li>AI-driven music composition.</li>
						<li>AI-assisted sound design.</li>
						<li>AI-generated audio and video.</li>
						<li>Legal and ethical considerations of AI in music.</li>
					</ul>
				</section>

				<!-- Schedule -->
				<section>
					<h2>Schedule</h2>

					<table class="modern-table">
						<tr>
							<th>Time</th>
							<th>Topic</th>
						</tr>

						<tr>
							<td>09:00AM</td>
							<td>Welcome by Organizers</td>
						</tr>

						<tr>
							<td>09:10AM</td>
							<td>Invited Speech by Zhiyao Duan</td>
						</tr>

						<tr>
							<td>09:50AM</td>
							<td>Invited Speech by Miguel Willis</td>
						</tr>

						<tr>
							<td>10:30AM</td>
							<td>Break</td>
						</tr>

						<tr>
							<td>10:40AM</td>
							<td>
								Paper Presentations
								<ol class="numbered-list">
									<li>Evaluating Interval-based Tokenization for Pitch Representation in Symbolic
										Music Analysis</li>
									<li>Frechet Music Distance: A Metric For Generative Symbolic Music Evaluation</li>
									<li>M2M-Gen: A Multimodal Framework for Automated Background Music Generation in
										Japanese Manga Using Large Language Models</li>
									<li>AffectMachine-Pop: A controllable expert system for real-time pop music
										generation</li>
									<li>Understanding Unscripted Music Practice</li>
									<li>Hidden Echoes Survive Training in Audio To Audio Generative Instrument Models
									</li>
									<li>Revisiting Your Memory: Reconstruction of Affect-Contextualized Memory via
										EEG-guided Audiovisual Generation</li>
									<li>MMVA: Multimodal Matching Based on Valence and Arousal across Images, Music, and
										Musical Captions</li>
									<li>Towards Music Industry 5.0: Perspectives on Artificial Intelligence</li>
								</ol>
							</td>
						</tr>

						<tr>
							<td>12:30PM</td>
							<td>Lunch Break</td>
						</tr>

						<tr>
							<td>01:00PM</td>
							<td>Invited Speech by Hao-Wen Dong</td>
						</tr>

						<tr>
							<td>01:40PM</td>
							<td>Invited Speech by Gus Xia</td>
						</tr>

						<tr>
							<td>02:20PM</td>
							<td>Panel Discussion by the Invited Speakers</td>
						</tr>

						<tr>
							<td>03:20PM</td>
							<td>Break</td>
						</tr>

						<tr>
							<td>03:30PM</td>
							<td>
								Poster Presentations and Participant Interaction
								<ol class="numbered-list">
									<li>Evaluating Interval-based Tokenization for Pitch Representation in Symbolic
										Music Analysis</li>
									<li>Frechet Music Distance: A Metric For Generative Symbolic Music Evaluation</li>
									<li>M2M-Gen: A Multimodal Framework for Automated Background Music Generation in
										Japanese Manga Using Large Language Models</li>
									<li>AffectMachine-Pop: A controllable expert system for real-time pop music
										generation</li>
									<li>Understanding Unscripted Music Practice</li>
									<li>Hidden Echoes Survive Training in Audio To Audio Generative Instrument Models
									</li>
									<li>Revisiting Your Memory: Reconstruction of Affect-Contextualized Memory via
										EEG-guided Audiovisual Generation</li>
									<li>MMVA: Multimodal Matching Based on Valence and Arousal across Images, Music, and
										Musical Captions</li>
									<li>Towards Music Industry 5.0: Perspectives on Artificial Intelligence</li>
								</ol>
							</td>
						</tr>

						<tr>
							<td>04:30PM</td>
							<td>Open Discussion: Future of AI and Music</td>
						</tr>

						<tr>
							<td>05:00PM</td>
							<td>Adjourn</td>
						</tr>
					</table>
				</section>

				<!-- Accepted Papers -->
				<section>
					<h2>Accepted Papers</h2>
					<div class="papers-container">
						<div class="paper-entry">
							<div class="paper-title">
								<a href="https://drive.google.com/file/d/1zhaSfIQ1ynxFosG2MNScrDLm0VFyzwkd/view?usp=drive_link"
									target="_blank">
									<strong>Evaluating Interval-based Tokenization for Pitch Representation in Symbolic
										Music Analysis</strong>
								</a>
								<a href="https://drive.google.com/file/d/1zhaSfIQ1ynxFosG2MNScrDLm0VFyzwkd/view?usp=drive_link"
									target="_blank">
									<img src="images/pdf-icon.png" alt="PDF" width="16" height="16" />
								</a>
							</div>
							<div class="paper-authors">
								Dinh-Viet-Toan Le (Université de Lille)*; Louis Bigo (Université de Bordeaux); Mikaela
								Keller (University of Lille)
							</div>
						</div>

						<div class="paper-entry">
							<div class="paper-title">
								<a href="https://drive.google.com/file/d/107dMD1QhrXIIreUsSMMVk3LuIPl_kpOU/view?usp=drive_link"
									target="_blank">
									<strong>Frechet Music Distance: A Metric For Generative Symbolic Music
										Evaluation</strong>
								</a>
								<a href="https://drive.google.com/file/d/107dMD1QhrXIIreUsSMMVk3LuIPl_kpOU/view?usp=drive_link"
									target="_blank">
									<img src="images/pdf-icon.png" alt="PDF" width="16" height="16" />
								</a>
							</div>
							<div class="paper-authors">
								Jan Retkowski (Warsaw University of Technology)*; Jakub Stępniak (Warsaw University of
								Technology); Mateusz Modrzejewski (Warsaw University of Technology)<br>
								<a href="https://github.com/jryban/frechet-music-distance"
									target="_blank">https://github.com/jryban/frechet-music-distance</a>
							</div>
						</div>

						<div class="paper-entry">
							<div class="paper-title">
								<a href="https://drive.google.com/file/d/1itoIlMlVFULwGj6MCL_Cm1e5MkfkMj1b/view?usp=drive_link"
									target="_blank">
									<strong>M2M-Gen: A Multimodal Framework for Automated Background Music Generation in
										Japanese Manga Using Large Language Models</strong>
								</a>
								<a href="https://drive.google.com/file/d/1itoIlMlVFULwGj6MCL_Cm1e5MkfkMj1b/view?usp=drive_link"
									target="_blank">
									<img src="images/pdf-icon.png" alt="PDF" width="16" height="16" />
								</a>
							</div>
							<div class="paper-authors">
								Megha Sharma (The University of Tokyo)*; Muhammad Haseeb (Mohamed bin Zayed University
								of Artificial Intelligence); Guangyu Xia (NYU Shanghai); Yoshimasa Tsuruoka (University
								of Tokyo)<br>
								<a href="https://manga-to-music.github.io/M2M-Gen/"
									target="_blank">https://manga-to-music.github.io/M2M-Gen/</a>
							</div>
						</div>

						<div class="paper-entry">
							<div class="paper-title">
								<a href="https://drive.google.com/file/d/1TzGUgBzHXF8CppaG12AiaEl_QljJMdiH/view?usp=drive_link"
									target="_blank">
									<strong>AffectMachine-Pop: A controllable expert system for real-time pop music
										generation</strong>
								</a>
								<a href="https://drive.google.com/file/d/1TzGUgBzHXF8CppaG12AiaEl_QljJMdiH/view?usp=drive_link"
									target="_blank">
									<img src="images/pdf-icon.png" alt="PDF" width="16" height="16" />
								</a>
							</div>
							<div class="paper-authors">
								Kat Agres (National University of Singapore)*; Adyasha Dash (National University of
								Singapore); Phoebe Chua (National University of Singapore); Stefan Ehrlich (SETLabs
								Research GmbH)
							</div>
						</div>

						<div class="paper-entry">
							<div class="paper-title">
								<strong>Understanding Unscripted Music Practice</strong>
							</div>
							<div class="paper-authors">
								Christopher Raphael (Indiana University)*
							</div>
						</div>

						<div class="paper-entry">
							<div class="paper-title">
								<a href="https://drive.google.com/file/d/1-0-vfPSfbZzyGQVsBOuhSinQfA8oUsEM/view?usp=drive_link"
									target="_blank">
									<strong>Hidden Echoes Survive Training in Audio To Audio Generative Instrument
										Models</strong>
								</a>
								<a href="https://drive.google.com/file/d/1-0-vfPSfbZzyGQVsBOuhSinQfA8oUsEM/view?usp=drive_link"
									target="_blank">
									<img src="images/pdf-icon.png" alt="PDF" width="16" height="16" />
								</a>
							</div>
							<div class="paper-authors">
								Christopher Tralie (Ursinus College)*; Matt Amery (Create And Innovate UK); Ben Douglas
								(Ursinus College); Ian Utz (Ursinus College)<br>
								<a href="https://www.ctralie.com/echoes/"
									target="_blank">https://www.ctralie.com/echoes/</a>
							</div>
						</div>

						<div class="paper-entry">
							<div class="paper-title">
								<a href="https://drive.google.com/file/d/19Q42QTnFvsRiRyJelPJEAIMH-tYDqPd6/view?usp=drive_link"
									target="_blank">
									<strong>Revisiting Your Memory: Reconstruction of Affect-Contextualized Memory via
										EEG-guided Audiovisual Generation</strong>
								</a>
								<a href="https://drive.google.com/file/d/19Q42QTnFvsRiRyJelPJEAIMH-tYDqPd6/view?usp=drive_link"
									target="_blank">
									<img src="images/pdf-icon.png" alt="PDF" width="16" height="16" />
								</a>
							</div>
							<div class="paper-authors">
								Joonwoo Kwon (Seoul National University)*; Heehwan Wang (Seoul National University);
								Jinwoo Lee (Seoul National University); Sooyoung Kim (Seoul National University);
								Shinjae Yoo (Brookhaven National Laboratory); Yuewei Lin (Brookhaven National
								Laboratory); Jiook Cha (Seoul National University)<br>
								<a href="https://github.com/ioahKwon/Revisiting-Your-Memory"
									target="_blank">https://github.com/ioahKwon/Revisiting-Your-Memory</a>
							</div>
						</div>

						<div class="paper-entry">
							<div class="paper-title">
								<a href="https://drive.google.com/file/d/1_nPCi--P-v2qecwQTHydzf65TIiesrM-/view?usp=drive_link"
									target="_blank">
									<strong>MMVA: Multimodal Matching Based on Valence and Arousal across Images, Music,
										and Musical Captions</strong>
								</a>
								<a href="https://drive.google.com/file/d/1_nPCi--P-v2qecwQTHydzf65TIiesrM-/view?usp=drive_link"
									target="_blank">
									<img src="images/pdf-icon.png" alt="PDF" width="16" height="16" />
								</a>
							</div>
							<div class="paper-authors">
								Suhwan Choi (Crabs.ai)*; Kyu Won Kim (Independent); Myungjoo Kang (Seoul National
								University)
							</div>
						</div>

						<div class="paper-entry">
							<div class="paper-title">
								<a href="https://drive.google.com/file/d/15Db_SEPuvXoKsMQR5HJsZuLM-Ov7U8Uj/view?usp=drive_link"
									target="_blank">
									<strong>Towards Music Industry 5.0: Perspectives on Artificial Intelligence</strong>
								</a>
								<a href="https://drive.google.com/file/d/15Db_SEPuvXoKsMQR5HJsZuLM-Ov7U8Uj/view?usp=drive_link"
									target="_blank">
									<img src="images/pdf-icon.png" alt="PDF" width="16" height="16" />
								</a>
							</div>
							<div class="paper-authors">
								Alexander Williams (Queen Mary University of London)*; Stefan Lattner (Sony CSL);
								Mathieu Barthet (Queen Mary University of London)<br>
								<a href="https://www.aim.qmul.ac.uk/" target="_blank">https://www.aim.qmul.ac.uk/</a>
							</div>
						</div>
					</div>
				</section>

				<!-- Call for Papers -->
				<section id="call-for-papers">
					<div class="inner">
						<h2>Call for Papers</h2>

						<h3>Submission Requirements</h3>
						<p>
							Submissions should be a maximum of 6 pages without references.
							Work in progress is welcome. Authors are encouraged to include
							descriptions of their prototype implementations. Additionally,
							authors are encouraged to interact with workshop attendees by
							including posters or demonstrations at the end of the workshop.
							Conceptual designs without any evidence of practical
							implementation are discouraged.
						</p>

						<p>
							Papers must be formatted in AAAI two-column, camera-ready style;
							see the AAAI-25 author kit for details. Papers must be in
							trouble-free, high-resolution PDF format, formatted for US
							Letter (8.5″ x 11″) paper, using Type 1 or TrueType fonts. AAAI
							submissions are anonymous and must conform to the instructions
							(detailed below) for double-blind review. The authors must
							remove all author and affiliation information from their
							submission for review, and may replace it with other
							information, such as paper number and keywords. Submissions may
							consist of up to 6 pages of technical content plus additional
							pages solely for references; acknowledgements should be omitted
							from papers submitted for review. Only PDF files are required at
							the time of submission for review.
						</p>

						<h3>Topics of Interest</h3>
						<ul>
							<li>AI-Driven Music Composition and Generation</li>
							<li>AI in Music Practice and Performance</li>
							<li>AI-based Music Recognition and Transcription</li>
							<li>AI Applications in Sound Design</li>
							<li>AI-Generated Videos to Accompany Music</li>
							<li>AI-Generated Lyrics Based on Music</li>
							<li>Legal or Ethical Implications of AI on Music</li>
							<li>AI's Impacts on Musicians' Careers</li>
							<li>AI Assisted Music Education</li>
							<li>Business Opportunities of AI and Music</li>
							<li>Music Datasets and Data Analysis</li>
						</ul>

						<h3>Paper Format</h3>
						<p>
							Please follow the format required by AAAI at
							<a href="https://aaai.org/conference/aaai/aaai-25/aaai-25-main-technical-call-for-papers/"
								target="_blank" rel="noopener noreferrer">
								AAAI 2025 Main Technical Call for Papers </a>.
						</p>

						<h3>Submission Site Information</h3>
						<p>
							Please submit your papers through the
							<a href="https://cmt3.research.microsoft.com/AI4Music2025" target="_blank"
								rel="noopener noreferrer">CMT Submission Portal</a>.
						</p>

						<h3>Important Dates</h3>
						<ul>
							<li><strong>Submission Deadline:</strong> November 22, 2024</li>
							<li>
								<strong>Notification of Acceptance:</strong> December 9, 2024
							</li>
							<li><strong>Final Version Due:</strong> December 31, 2024</li>
						</ul>

						<p>Accepted papers will be posted on the workshop website.</p>
					</div>
				</section>
				<!-- Invited Speakers -->
				<section>
					<h2>Invited Speakers</h2>

					<section class="wrapper style1 spotlights">
						<!-- Speaker 1: Hao-Wen (Herman) Dong -->
						<div class="team-member">
							<a href="https://hermandong.com/" target="_blank" rel="noopener noreferrer">
								<div class="team-photo">
									<img src="https://hermandong.com/profile.jpg" alt="Hao-Wen (Herman) Dong"
										class="profile-img">
								</div>
							</a>
							<div class="team-info">
								<h2>
									<a href="https://hermandong.com/" target="_blank" rel="noopener noreferrer">Hao-Wen
										(Herman) Dong</a>
								</h2>
								<p>
									Hao-Wen (Herman) Dong is an Assistant Professor in the
									Performing Arts Technology Department at the University of
									Michigan. Herman's research aims to empower music and audio
									creation with machine learning. His long-term goal is to
									lower the barrier of entry for music composition and
									democratize audio content creation. He is broadly interested
									in music generation, audio synthesis, multimodal machine
									learning, and music information retrieval. Herman received
									his PhD degree in Computer Science from the University of
									California San Diego, where he worked with Julian McAuley
									and Taylor Berg-Kirkpatrick. His research has been
									recognized by the UCSD CSE Doctoral Award for Excellence in
									Research, KAUST Rising Stars in AI, UChicago and UCSD Rising
									Stars in Data Science, ICASSP Rising Stars in Signal
									Processing, and UCSD GPSA Interdisciplinary Research Award.
								</p>
							</div>
						</div>

						<!-- Speaker 2: Zhiyao Duan -->
						<div class="team-member">
							<a href="https://www.hajim.rochester.edu/ece/people/faculty/duan_zhiyao/index.html"
								class="image" target="_blank" rel="noopener noreferrer">
								<div class="team-photo">
									<img src="https://hajim.rochester.edu/ece/sites/zduan/resource/ZhiyaoDuan2018_web.jpg"
										alt="Zhiyao Duan" />
								</div>
							</a>
							<div class="team-info">
								<h2>
									<a href="https://www.hajim.rochester.edu/ece/people/faculty/duan_zhiyao/index.html"
										target="_blank" rel="noopener noreferrer">Zhiyao Duan</a>
								</h2>
								<p>
									Zhiyao Duan is an associate professor in Electrical and
									Computer Engineering, Computer Science, and Data Science at
									the University of Rochester. He is also a co-founder of
									Violy, a company aiming to improve music education through
									AI. His research interest is in computer audition and its
									connections with computer vision, natural language
									processing, and augmented and virtual reality. He received a
									best paper award at the Sound and Music Computing (SMC)
									Conference in 2017, a best paper nomination at the
									International Society for Music Information Retrieval
									(ISMIR) Conference in 2017, and a CAREER award from the
									National Science Foundation (NSF). His work has been funded
									by NSF, National Institute of Health, National Institute of
									Justice, New York State Center of Excellence in Data
									Science, and University of Rochester internal awards on
									AR/VR, health analytics, and data science. He is a senior
									area editor of IEEE Signal Processing Letters, an associate
									editor for IEEE Open Journal of Signal Processing, and a
									guest editor for Transactions of the International Society
									for Music Information Retrieval. He is the President of
									ISMIR.
								</p>
							</div>
						</div>

						<!-- Speaker 3: Miguel Willis -->
						<div class="team-member">
							<a href="https://www.law.upenn.edu/faculty/willism1" class="image" target="_blank"
								rel="noopener noreferrer">
								<div class="team-photo">
									<img src="https://www.law.upenn.edu/live/image/gid/35/width/518/height/576/crop/1/src_region/0,0,663,664/43301_Miguel_Willis.rev.1646392650.png"
										alt="Miguel Willis" />
								</div>
							</a>
							<div class="team-info">
								<h2>
									<a href="https://www.law.upenn.edu/faculty/willism1" target="_blank"
										rel="noopener noreferrer">Miguel Willis</a>
								</h2>
								<p>
									Miguel Willis is the Innovator in Residence at the Law
									School's Future of the Profession Initiative (FPI),
									University of Pennsylvania. He concurrently serves as the
									Executive Director of Access to Justice Tech Fellows, a
									national nonprofit organization that develops summer
									fellowships for law students seeking to leverage
									technology to create equitable legal access for low-income
									and marginalized populations. Prior to joining FPI, Willis
									served as the Law School Admissions Council's (LSAC)
									inaugural Presidential Innovation Fellow. Willis currently
									serves on the advisory board of the University of Arizona
									James E. Rogers College of Law's Innovation for Justice
									(i4J) program and serves on The Legal Services
									Corporation's Emerging Leaders Council.
								</p>
							</div>
						</div>

						<!-- Speaker 4: Gus Xia -->
						<div class="team-member">
							<a href="https://mbzuai.ac.ae/study/faculty/dr-gus-xia/" class="image" target="_blank"
								rel="noopener noreferrer">
								<div class="team-photo">
									<img src="https://mbzuai.ac.ae/wp-content/uploads/2022/06/profile_gus-xia_secondary.jpg"
										alt="Gus Xia" />
								</div>
							</a>
							<div class="team-info">
								<h2>
									<a href="https://mbzuai.ac.ae/study/faculty/dr-gus-xia/" target="_blank"
										rel="noopener noreferrer">Gus Xia</a>
								</h2>
								<p>
									Gus Xia is an assistant professor of Machine Learning at the
									Mohamed bin Zayed University of Artificial Intelligence in
									Masdar City, Abu Dhabi. His research includes the design of
									interactive intelligent systems to extend human musical
									creation and expression. This research lies at the
									intersection of machine learning, human-computer
									interaction, robotics, and computer music. Some
									representative works include interactive composition via
									style transfer, human-computer interactive performances,
									autonomous dancing robots, large-scale content-based music
									retrieval, haptic guidance for flute tutoring, and bio-music
									computing using slime mold.
								</p>
							</div>
						</div>
					</section>
				</section>

				<!-- Organizers -->
				<section>
					<h2>Organizers</h2>
					<p>
						Meet the team behind the 2025 AAAI Workshop on Artificial
						Intelligence for Music.
					</p>
					<section id="organizers" class="wrapper style1 spotlights">
						<!-- Organizer 1 -->
						<div class="team-member">
							<div class="team-photo">
								<img src="https://engineering.purdue.edu/ResourceDB/ResourceFiles/image277153"
									alt="Yung Hsiang Lu" class="profile-img" />
							</div>
							<div class="team-info">
								<h2>
									<a href="https://yhlu.net/" target="_blank" rel="noopener noreferrer">Yung-Hsiang
										Lu</a>
								</h2>
								<h3>Professor of Electrical and Computer Engineering</h3>
								<p>
									Yung-Hsiang Lu is a professor in the Elmore Family School of
									Electrical and Computer Engineering at Purdue University. He
									is a fellow of the IEEE and a distinguished scientist of the
									ACM. Yung-Hsiang has published papers on computer vision and
									machine learning in venues such as AI Magazine, Nature
									Machine Learning, and Computer. He is one of the editors of
									the book "Low-Power Computer Vision: Improve the Efficiency
									of Artificial Intelligence" (ISBN 9780367744700, 2022 by
									Chapman & Hall).
								</p>
							</div>
						</div>

						<!-- Organizer 2 -->
						<div class="team-member">
							<div class="team-photo">
								<img src="images/yun.jpg" alt="Dr. Kristen Yeon-Ji Yun" class="profile-img" />
							</div>
							<div class="team-info">
								<h2>
									<a href="https://kristenyeonjiyun.com/" target="_blank"
										rel="noopener noreferrer">Kristen Yeon-Ji Yun</a>
								</h2>
								<h3>Clinical Associate Professor of Music</h3>
								<p>
									Kristen Yeon-Ji Yun is a clinical associate professor in the
									Department of Music at the Patti and Rusty Rueff School of
									Design, Art, and Performance at Purdue University. She is
									the Principal Investigator of the research project
									"Artificial Intelligence Technology for Future Music
									Performers" (US National Science Foundation, IIS 2326198).
									Kristen is an active soloist, chamber musician, musical
									scholar, and clinician. She has toured many countries,
									including Malaysia, Thailand, Germany, Mexico, Japan, China,
									Hong Kong, Spain, France, Italy, Taiwan, and South Korea,
									giving a series of successful concerts and master classes.
								</p>
							</div>
						</div>

						<!-- Organizer 3 -->
						<div class="team-member">
							<div class="team-photo">
								<img src="https://www.luc.edu/media/lucedu/computerscience/images/newbwfacultyphotos/George_Thiruvathukal.jpg"
									alt="George K. Thiruvathukal" class="profile-img">
							</div>
							<div class="team-info">
								<h2>
									<a href="https://gkt.sh/" target="_blank" rel="noopener noreferrer">George K.
										Thiruvathukal</a>
								</h2>
								<h3>Professor and Chairperson of Computer Science</h3>
								<p>
									George K. Thiruvathukal is a professor and chairperson of
									Computer Science at Loyola University Chicago and a
									visiting computer scientist at Argonne National
									Laboratory. His research interests include
									high-performance computing and distributed systems,
									programming languages, software engineering, machine
									learning, digital humanities, and arts (primarily music).
									George has published multiple books, including "Software
									Engineering for Science" (ISBN 9780367574277, 2016 Chapman
									and Hall & CRC), "Web Programming: Techniques for
									Integrating Python, Linux, Apache, and MySQL" (ISBN
									9780130410658, 2001 Prentice Hall), and "High-Performance
									Java Platform Computing: Multithreaded and Networked
									Programming" (ISBN 9780130161642, 2000 Prentice Hall).
								</p>
							</div>
						</div>

						<!-- Organizer 4 -->
						<div class="team-member">
							<div class="team-photo">
								<img src="images/benchou.jpg" alt="Benjamin Shiue-Hal Chou" class="profile-img">
							</div>
							<div class="team-info">
								<h2>
									<a href="https://www.linkedin.com/in/benjamin-chou-6aa058228/" target="_blank"
										rel="noopener noreferrer">Benjamin Shiue-Hal Chou</a>
								</h2>
								<h3>PhD Student and Lab Graduate Mentor</h3>
								<p>
									Benjamin Shiue-Hal Chou is a PhD student in Electrical and
									Computer Engineering at Purdue University, supervised by
									Dr. Yung-Hsiang Lu. His research focuses on artificial
									intelligence applications in music technology,
									particularly on detecting errors in music performances.
									Benjamin has co-authored “Token Turing Machines are
									Efficient Vision Models” (arXiv preprint arXiv:2409.07613,
									2024). He earned his Bachelor of Science in Electrical
									Engineering from National Cheng Kung University (NCKU) in
									Taiwan, receiving awards such as the Outstanding Student
									Scholarship, Transnational Research Scholarship Grant, and
									the Tainan City Digital Governance Talent Award.
								</p>
							</div>
						</div>

						<!-- Add additional organizers following the same structure -->
					</section>
				</section>
			</div>
		</section>
	</div>

	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/jquery.scrollex.min.js"></script>
	<script src="assets/js/jquery.scrolly.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>
</body>

</html>
