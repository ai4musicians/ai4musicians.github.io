<!DOCTYPE html>
<html lang="en" xml:lang="en">

<head>
	<title>2025 AAAI Workshop AI for Music</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<meta name="description"
		content="Join the 2025 AAAI Workshop on Artificial Intelligence for Music, exploring the intersection of AI and music creation, recognition, education, and more." />
	<link rel="stylesheet" href="assets/css/main.css" />
	<noscript>
		<link rel="stylesheet" href="assets/css/noscript.css" />
	</noscript>
</head>

<body class="is-preload">
	<!-- Sidebar -->
	<section id="sidebar">
		<div class="inner">
			<nav>
				<ul>
					<li>
						<a href="index.html"><img src="images/aim.png" width="50" alt="AIM Logo" /></a>
					</li>
					<li><a href="index.html#news">News</a></li>
					<li><a href="index.html#amt">Our Projects</a></li>
					<li><a href="index.html#activities">Research Areas</a></li>
					<li><a href="index.html#who">Leaders</a></li>
					<li><a href="index.html#outreach">Outreach</a></li>
					<li><a href="index.html#vipteam">VIP Team</a></li>
					<li><a href="index.html#contact">Contact</a></li>
					<li><a href="#call-for-papers">Call for Papers</a></li>
					<!-- Added Navigation Link -->
				</ul>
			</nav>
		</div>
	</section>

	<!-- Wrapper -->
	<div id="wrapper">
		<!-- Main -->
		<section id="main" class="wrapper style1 fullscreen fade-up">
			<div class="inner">
				<h1 class="major">Artificial Intelligence for Music</h1>

				<p class="subtitle">
					A workshop at
					<a href="https://aaai.org/conference/aaai/aaai-25/" target="_blank" rel="noopener noreferrer">
						2025 AAAI Annual Conference
					</a>
				</p>

				<!-- Summary -->

				<section>
					<h2>2025/03/03 Monday</h2>
					<h2>Workshop Summary</h2>
					<p>
						This one-day workshop will explore the dynamic intersection of
						artificial intelligence and music. It explores how AI is
						transforming music creation, recognition, and education, ethical
						and legal implications, as well as business opportunities. We will
						investigate how AI is changing the music industry and
						education—from composition to performance, production,
						collaboration, and audience experience. Participants will gain
						insights into the technological challenges in music and how AI can
						enhance creativity, enabling musicians and producers to push the
						boundaries of their art. The workshop will cover topics such as
						AI-driven music composition, where algorithms generate melodies,
						harmonies, and even full orchestral arrangements. We will discuss
						how AI tools assist in sound design, remixing, and mastering,
						allowing for new sonic possibilities and efficiencies in music
						production. Additionally, we'll examine AI's impact on music
						education and the careers of musicians, exploring advanced
						learning tools and teaching methods. AI technologies are
						increasingly adopted in the music and entertainment industry. The
						workshop will also discuss the legal and ethical implications of
						AI in music, including questions of authorship, originality, and
						the evolving role of human artists in an increasingly automated
						world. This workshop is designed for AI researchers, musicians,
						producers, and educators interested in the current status and
						future of AI in music.
					</p>
				</section>

				<section>
					<h2>Topics</h2>
					<ul>
						<li>
							Impacts of AI on music education and careers of musicians.
						</li>
						<li>AI-driven music composition.</li>
						<li>AI-assisted sound design.</li>
						<li>AI-generated audio and video.</li>
						<li>Legal and ethical considerations of AI in music.</li>
					</ul>
				</section>

				<!-- Schedule -->
				<section>
					<h2>Schedule</h2>

					<table class="modern-table">
						<tr>
							<th>Time</th>
							<th>Topic</th>
						</tr>

						<tr>
							<td>09:00AM</td>
							<td>Welcome by Organizers</td>
						</tr>

						<tr>
							<td>09:10AM</td>
							<td>Invited Speech by Zhiyao Duan</td>
						</tr>

						<tr>
							<td>09:50AM</td>
							<td>Invited Speech by Miguel Willis</td>
						</tr>

						<tr>
							<td>10:30AM</td>
							<td>Break</td>
						</tr>

						<tr>
							<td>10:40AM</td>
							<td>
								Paper Presentations
								<ol class="numbered-list">
									<li>Evaluating Interval-based Tokenization for Pitch Representation in Symbolic
										Music Analysis</li>
									<li>Frechet Music Distance: A Metric For Generative Symbolic Music Evaluation</li>
									<li>M2M-Gen: A Multimodal Framework for Automated Background Music Generation in
										Japanese Manga Using Large Language Models</li>
									<li>AffectMachine-Pop: A controllable expert system for real-time pop music
										generation</li>
									<li>Understanding Unscripted Music Practice</li>
									<li>Hidden Echoes Survive Training in Audio To Audio Generative Instrument Models
									</li>
									<li>Revisiting Your Memory: Reconstruction of Affect-Contextualized Memory via
										EEG-guided Audiovisual Generation</li>
									<li>MMVA: Multimodal Matching Based on Valence and Arousal across Images, Music, and
										Musical Captions</li>
									<li>Towards Music Industry 5.0: Perspectives on Artificial Intelligence</li>
								</ol>
							</td>
						</tr>

						<tr>
							<td>12:30PM</td>
							<td>Lunch Break</td>
						</tr>

						<tr>
							<td>01:00PM</td>
							<td>Invited Speech by Hao-Wen Dong</td>
						</tr>

						<tr>
							<td>01:40PM</td>
							<td>Invited Speech by Gus Xia</td>
						</tr>

						<tr>
							<td>02:20PM</td>
							<td>Panel Discussion by the Invited Speakers</td>
						</tr>

						<tr>
							<td>03:20PM</td>
							<td>Break</td>
						</tr>

						<tr>
							<td>03:30PM</td>
							<td>
								Poster Presentations and Participant Interaction
								<ol class="numbered-list">
									<li>Evaluating Interval-based Tokenization for Pitch Representation in Symbolic
										Music Analysis</li>
									<li>Frechet Music Distance: A Metric For Generative Symbolic Music Evaluation</li>
									<li>M2M-Gen: A Multimodal Framework for Automated Background Music Generation in
										Japanese Manga Using Large Language Models</li>
									<li>AffectMachine-Pop: A controllable expert system for real-time pop music
										generation</li>
									<li>Understanding Unscripted Music Practice</li>
									<li>Hidden Echoes Survive Training in Audio To Audio Generative Instrument Models
									</li>
									<li>Revisiting Your Memory: Reconstruction of Affect-Contextualized Memory via
										EEG-guided Audiovisual Generation</li>
									<li>MMVA: Multimodal Matching Based on Valence and Arousal across Images, Music, and
										Musical Captions</li>
									<li>Towards Music Industry 5.0: Perspectives on Artificial Intelligence</li>
								</ol>
							</td>
						</tr>

						<tr>
							<td>04:30PM</td>
							<td>Open Discussion: Future of AI and Music</td>
						</tr>

						<tr>
							<td>05:00PM</td>
							<td>Adjourn</td>
						</tr>
					</table>
				</section>

				<!-- Accepted Papers -->
				<section>
					<h2>Accepted Papers</h2>
					<div class="papers-container">
						<div class="paper-entry">
							<div class="paper-title">
								<a href="https://drive.google.com/file/d/1zhaSfIQ1ynxFosG2MNScrDLm0VFyzwkd/view?usp=drive_link"
									target="_blank">
									<strong>Evaluating Interval-based Tokenization for Pitch Representation in Symbolic
										Music Analysis</strong>
								</a>
								<a href="https://drive.google.com/file/d/1zhaSfIQ1ynxFosG2MNScrDLm0VFyzwkd/view?usp=drive_link"
									target="_blank">
									<img src="images/pdf-icon.png" alt="PDF" width="16" height="16" />
								</a>
							</div>
							<div class="paper-authors">
								Dinh-Viet-Toan Le (Université de Lille)*; Louis Bigo (Université de Bordeaux); Mikaela
								Keller (University of Lille)
							</div>
							<div>
								Symbolic music analysis tasks are often performed by models
								originally developed for Natural Language Processing, such as Transformers. Such models require the input data to be represented as sequences, which is achieved through a process of tokenization. Tokenization strategies for symbolic music often rely on absolute MIDI values to represent pitch information. However, music research largely promotes the benefit of higher-level representations such as melodic contour and harmonic relations for which pitch intervals turn out to
								be more expressive than absolute pitches. In this work, we introduce a general framework for building interval-based tokenizations. By evaluating these tokenizations on three music analysis tasks, we show that such interval-based tokenizations improve model performances and facilitate their explainability.
							</div>
						</div>

						<div class="paper-entry">
							<div class="paper-title">
								<a href="https://drive.google.com/file/d/107dMD1QhrXIIreUsSMMVk3LuIPl_kpOU/view?usp=drive_link"
									target="_blank">
									<strong>Frechet Music Distance: A Metric For Generative Symbolic Music
										Evaluation</strong>
								</a>
								<a href="https://drive.google.com/file/d/107dMD1QhrXIIreUsSMMVk3LuIPl_kpOU/view?usp=drive_link"
									target="_blank">
									<img src="images/pdf-icon.png" alt="PDF" width="16" height="16" />
								</a>
							</div>
							<div class="paper-authors">
								Jan Retkowski (Warsaw University of Technology)*; Jakub Stępniak (Warsaw University of
								Technology); Mateusz Modrzejewski (Warsaw University of Technology)<br>
								<a href="https://github.com/jryban/frechet-music-distance"
									target="_blank">https://github.com/jryban/frechet-music-distance</a>
							</div>
							<div>
								In this paper we introduce the Frechet Music Distance (FMD), a novel evaluation metric for generative symbolic music models, inspired by the Frechet Inception Distance (FID) in computer vision and Frechet Audio Distance (FAD) in generative audio. FMD calculates the distance between distributions of reference and generated symbolic music embeddings, capturing abstract musical features. We validate FMD across several datasets and models. Results indicate that FMD effectively differentiates model quality, providing a domain specific metric for evaluating symbolic music generation and establishing a reproducible standard for future research in symbolic music modeling.
							</div>
						</div>

						<div class="paper-entry">
							<div class="paper-title">
								<a href="https://drive.google.com/file/d/1itoIlMlVFULwGj6MCL_Cm1e5MkfkMj1b/view?usp=drive_link"
									target="_blank">
									<strong>M2M-Gen: A Multimodal Framework for
										Automated Background Music Generation in
										Japanese Manga Using Large Language Models</strong>
								</a>
								<a href="https://drive.google.com/file/d/1itoIlMlVFULwGj6MCL_Cm1e5MkfkMj1b/view?usp=drive_link"
									target="_blank">
									<img src="images/pdf-icon.png" alt="PDF" width="16" height="16" />
								</a>
							</div>
							<div class="paper-authors">
								Megha Sharma (The University of Tokyo)*; Muhammad Haseeb (Mohamed bin Zayed University
								of Artificial Intelligence); Guangyu Xia (NYU Shanghai); Yoshimasa Tsuruoka (University
								of Tokyo)<br>
								<a href="https://manga-to-music.github.io/M2M-Gen/"
									target="_blank">https://manga-to-music.github.io/M2M-Gen/</a>
							</div>
							<div>
								This paper introduces M2M Gen (Manga to Music Generation), a multi-modal framework for generating background music tailored to Japanese manga. The key challenges in this task are the lack of an available dataset or a baseline. To address these challenges, we propose an automated music generation pipeline that produces background music for an input manga book. Initially, we use the dialogues in a manga to detect scene boundaries and perform emotion classification using the characters’ faces within a scene. Then, we use GPT 4o to translate this low level scene information into a high level music directive. Conditioned on the scene information and the music directive, another instance of GPT 4o generates page level music captions to guide a text to music model. This produces music that is aligned with the manga’s evolving narrative. The effectiveness of M2M Gen is confirmed through extensive subjective evaluations, showcasing its capability to generate higher quality, more relevant and consistent music that complements specific scenes when compared to our baselines.
							</div>
						</div>

						<div class="paper-entry">
							<div class="paper-title">
								<a href="https://drive.google.com/file/d/1TzGUgBzHXF8CppaG12AiaEl_QljJMdiH/view?usp=drive_link"
									target="_blank">
									<strong>AffectMachine-Pop: A controllable expert system for real-time pop music generation</strong>
								</a>
								<a href="https://drive.google.com/file/d/1TzGUgBzHXF8CppaG12AiaEl_QljJMdiH/view?usp=drive_link"
									target="_blank">
									<img src="images/pdf-icon.png" alt="PDF" width="16" height="16" />
								</a>
							</div>
							<div class="paper-authors">
								Kat Agres (National University of Singapore)*; Adyasha Dash (National University of
								Singapore); Phoebe Chua (National University of Singapore); Stefan Ehrlich (SETLabs
								Research GmbH)
							</div>
							<div>
								Music is a powerful medium for influencing listeners’ emotional states, and this capacity has driven a surge of research interest in AI based affective music generation in recent years. Many existing systems, however, are a black box which are not directly controllable, thus making these systems less flexible and adaptive to users. We present AffectMachine Pop, an expert system capable of generating retro pop music according to arousal and valence values, which can either be pre determined or based on a listener’s real time emotion states. To validate the efficacy of the system, we conducted a listening study demonstrating that AffectMachine Pop is capable of generating affective music at target levels of arousal and valence. The system is tailored for use either as a tool for generating interactive affective music based on user input, or for incorporation into biofeedback or neurofeedback systems to assist users with emotion self regulation.
							</div>
						</div>

						<div class="paper-entry">
							<div class="paper-title">
								<strong>Understanding Unscripted Music Practice</strong>
							</div>
							<div class="paper-authors">
								Christopher Raphael (Indiana University)*
							</div>
						</div>

						<div class="paper-entry">
							<div class="paper-title">
								<a href="https://drive.google.com/file/d/1-0-vfPSfbZzyGQVsBOuhSinQfA8oUsEM/view?usp=drive_link"
									target="_blank">
									<strong>Hidden Echoes Survive Training in Audio To Audio Generative Instrument
										Models</strong>
								</a>
								<a href="https://drive.google.com/file/d/1-0-vfPSfbZzyGQVsBOuhSinQfA8oUsEM/view?usp=drive_link"
									target="_blank">
									<img src="images/pdf-icon.png" alt="PDF" width="16" height="16" />
								</a>
							</div>
							<div class="paper-authors">
								Christopher Tralie (Ursinus College)*; Matt Amery (Create And Innovate UK); Ben Douglas
								(Ursinus College); Ian Utz (Ursinus College)<br>
								<a href="https://www.ctralie.com/echoes/"
									target="_blank">https://www.ctralie.com/echoes/</a>
							</div>
							<div>
								As generative techniques pervade the audio domain, there has been increasing interest in tracing back through these complicated models to understand how they draw on their training data to synthesize new examples. This is important both to ensure they use properly licensed data and to elucidate their black box behavior.

In this paper, we show that if imperceptible echoes are hidden in the training data, a wide variety of audio to audio architectures, including differentiable digital signal processing (DDSP), Realtime Audio Variational AutoEncoder (RAVE), and “Dance Diffusion,” will reproduce these echoes in their outputs. Hiding a single echo is particularly robust across all architectures, and we also show promising results in hiding longer time spread echo patterns for increased information capacity.

We conclude by showing that echoes make their way into fine tuned models, survive mixing/demixing, and persist through pitch shift augmentation during training. This simple, classical idea in watermarking demonstrates significant promise for tagging generative audio models.
							</div>
						</div>

						<div class="paper-entry">
							<div class="paper-title">
								<a href="https://drive.google.com/file/d/19Q42QTnFvsRiRyJelPJEAIMH-tYDqPd6/view?usp=drive_link"
									target="_blank">
									<strong>Revisiting Your Memory: Reconstruction of Affect-Contextualized Memory via
										EEG-guided Audiovisual Generation</strong>
								</a>
								<a href="https://drive.google.com/file/d/19Q42QTnFvsRiRyJelPJEAIMH-tYDqPd6/view?usp=drive_link"
									target="_blank">
									<img src="images/pdf-icon.png" alt="PDF" width="16" height="16" />
								</a>
							</div>
							<div class="paper-authors">
								Joonwoo Kwon (Seoul National University)*; Heehwan Wang (Seoul National University);
								Jinwoo Lee (Seoul National University); Sooyoung Kim (Seoul National University);
								Shinjae Yoo (Brookhaven National Laboratory); Yuewei Lin (Brookhaven National
								Laboratory); Jiook Cha (Seoul National University)<br>
								<a href="https://github.com/ioahKwon/Revisiting-Your-Memory"
									target="_blank">https://github.com/ioahKwon/Revisiting-Your-Memory</a>
							</div>
							<div>
								In this paper, we introduce RecallAffectiveMemory, a novel task designed to reconstruct autobiographical memories through audio visual generation guided by affect extracted from electroencephalogram (EEG) signals. To support this task, we present the EEG AffectiveMemory dataset, which includes textual descriptions, visuals, music, and EEG recordings collected during memory recall from nine participants.

We also propose RYM (Recall Your Memory), a three-stage framework for generating synchronized audio visual content while maintaining dynamic personal memory affect trajectories. Experimental results show that our method can faithfully reconstruct affect contextualized audio visual memories across all subjects, both qualitatively and quantitatively, with participants reporting strong affective concordance between their recalled memories and the generated content. Our approach advances affect decoding research and its practical applications in personalized media creation through neural based affect comprehension.
							</div>
						</div>

						<div class="paper-entry">
							<div class="paper-title">
								<a href="https://drive.google.com/file/d/1_nPCi--P-v2qecwQTHydzf65TIiesrM-/view?usp=drive_link"
									target="_blank">
									<strong>MMVA: Multimodal Matching Based on Valence and Arousal across Images, Music,
										and Musical Captions</strong>
								</a>
								<a href="https://drive.google.com/file/d/1_nPCi--P-v2qecwQTHydzf65TIiesrM-/view?usp=drive_link"
									target="_blank">
									<img src="images/pdf-icon.png" alt="PDF" width="16" height="16" />
								</a>
							</div>
							<div class="paper-authors">
								Suhwan Choi (Crabs.ai)*; Kyu Won Kim (Independent); Myungjoo Kang (Seoul National
								University)
							</div>
							<div>
								We introduce Multimodal Matching based on Valence and Arousal (MMVA), a tri-modal encoder framework designed to capture emotional content across images, music, and musical captions. To support this framework, we expand the Image Music Emotion Matching Net (IMEMNet) dataset, creating IMEMNet C, which includes 24,756 images and 25,944 music clips with corresponding musical captions.

We employ multimodal matching scores based on continuous valence (emotional positivity) and arousal (emotional intensity) values. This continuous matching score enables random sampling of image music pairs during training by computing similarity scores from the valence arousal values across different modalities. Consequently, the proposed approach achieves state of the art performance in valence arousal prediction tasks. Furthermore, the framework demonstrates its efficacy in various zero shot tasks, highlighting the potential of valence and arousal predictions in downstream applications.
							</div>
						</div>

						<div class="paper-entry">
						  <div class="paper-title">

						    <a href="https://drive.google.com/file/d/1iGkd3htZ4sCTKMt28BIj3HuFR7_RtFb_/view?usp=sharing">

									<strong>Towards Music Industry 5.0: Perspectives on Artificial Intelligence</strong>
								</a>
								<a href="https://drive.google.com/file/d/15Db_SEPuvXoKsMQR5HJsZuLM-Ov7U8Uj/view?usp=drive_link"
									target="_blank">
									<img src="images/pdf-icon.png" alt="PDF" width="16" height="16" />
								</a>
							</div>
							<div class="paper-authors">
								Alexander Williams (Queen Mary University of London)*; 
								Mathieu Barthet (Queen Mary University of London,
								Aix-Marseille Univ CNRS PRISM)<br>
								<a href="https://www.aim.qmul.ac.uk/" target="_blank">https://www.aim.qmul.ac.uk/</a>
							</div>
							<div>
								Artificial Intelligence (AI) is a disruptive technology transforming many industries, including the music industry. Recently, the concept of Industry 5.0 has been proposed, emphasizing principles of sustainability, resilience, and human centricity to address shortcomings in Industry 4.0 and its associated technologies, including AI. In line with these principles, this paper advocates for ethical AI practices in the music industry.

We outline the current state of AI in the music industry and its broader ethical and legal issues through an analysis of contemporary case studies. We list current commercial applications of AI in music, gather perspectives from diverse stakeholders, and examine existing and forthcoming regulatory frameworks and industry initiatives. This paper provides timely research directions, practical recommendations, and commercial opportunities to aid the transition to a human centric, resilient, and sustainable music industry 5.0.

Our focus is primarily on case studies from the western music industry in the European Union (EU), United States of America (US), and United Kingdom (UK), though many of the issues discussed are universal. While not exhaustive, this work aims to guide researchers, businesses, and policymakers in developing responsible frameworks for deploying and regulating AI in the music industry.
							</div>
						</div>
					</div>
				</section>

				<!-- Call for Papers -->
				<section id="call-for-papers">
					<div class="inner">
						<h2>Call for Papers</h2>

						<h3>Submission Requirements</h3>
						<p>
							Submissions should be a maximum of 6 pages without references.
							Work in progress is welcome. Authors are encouraged to include
							descriptions of their prototype implementations. Additionally,
							authors are encouraged to interact with workshop attendees by
							including posters or demonstrations at the end of the workshop.
							Conceptual designs without any evidence of practical
							implementation are discouraged.
						</p>

						<p>
							Papers must be formatted in AAAI two-column, camera-ready style;
							see the AAAI-25 author kit for details. Papers must be in
							trouble-free, high-resolution PDF format, formatted for US
							Letter (8.5″ x 11″) paper, using Type 1 or TrueType fonts. AAAI
							submissions are anonymous and must conform to the instructions
							(detailed below) for double-blind review. The authors must
							remove all author and affiliation information from their
							submission for review, and may replace it with other
							information, such as paper number and keywords. Submissions may
							consist of up to 6 pages of technical content plus additional
							pages solely for references; acknowledgements should be omitted
							from papers submitted for review. Only PDF files are required at
							the time of submission for review.
						</p>

						<h3>Topics of Interest</h3>
						<ul>
							<li>AI-Driven Music Composition and Generation</li>
							<li>AI in Music Practice and Performance</li>
							<li>AI-based Music Recognition and Transcription</li>
							<li>AI Applications in Sound Design</li>
							<li>AI-Generated Videos to Accompany Music</li>
							<li>AI-Generated Lyrics Based on Music</li>
							<li>Legal or Ethical Implications of AI on Music</li>
							<li>AI's Impacts on Musicians' Careers</li>
							<li>AI Assisted Music Education</li>
							<li>Business Opportunities of AI and Music</li>
							<li>Music Datasets and Data Analysis</li>
						</ul>

						<h3>Paper Format</h3>
						<p>
							Please follow the format required by AAAI at
							<a href="https://aaai.org/conference/aaai/aaai-25/aaai-25-main-technical-call-for-papers/"
								target="_blank" rel="noopener noreferrer">
								AAAI 2025 Main Technical Call for Papers </a>.
						</p>

						<h3>Submission Site Information</h3>
						<p>
							Please submit your papers through the
							<a href="https://cmt3.research.microsoft.com/AI4Music2025" target="_blank"
								rel="noopener noreferrer">CMT Submission Portal</a>.
						</p>

						<h3>Important Dates</h3>
						<ul>
							<li><strong>Submission Deadline:</strong> November 22, 2024</li>
							<li>
								<strong>Notification of Acceptance:</strong> December 9, 2024
							</li>
							<li><strong>Final Version Due:</strong> December 31, 2024</li>
						</ul>

						<p>Accepted papers will be posted on the workshop website.</p>
					</div>
				</section>
				<!-- Invited Speakers -->
				<section>
					<h2>Invited Speakers</h2>

					<section class="wrapper style1 spotlights">
						<!-- Speaker 1: Hao-Wen (Herman) Dong -->
						<div class="team-member">
							<a href="https://hermandong.com/" target="_blank" rel="noopener noreferrer">
								<div class="team-photo">
									<img src="https://hermandong.com/headshot.jpg" alt="Hao-Wen (Herman) Dong"
										class="profile-img">
								</div>
							</a>
							<div class="team-info">
								<h2>
									<a href="https://hermandong.com/" target="_blank" rel="noopener noreferrer">Hao-Wen
										(Herman) Dong</a>
								</h2>
								<p>
									Hao-Wen (Herman) Dong is an Assistant Professor in the
									Performing Arts Technology Department at the University of
									Michigan. Herman's research aims to empower music and audio
									creation with machine learning. His long-term goal is to
									lower the barrier of entry for music composition and
									democratize audio content creation. He is broadly interested
									in music generation, audio synthesis, multimodal machine
									learning, and music information retrieval. Herman received
									his PhD degree in Computer Science from the University of
									California San Diego, where he worked with Julian McAuley
									and Taylor Berg-Kirkpatrick. His research has been
									recognized by the UCSD CSE Doctoral Award for Excellence in
									Research, KAUST Rising Stars in AI, UChicago and UCSD Rising
									Stars in Data Science, ICASSP Rising Stars in Signal
									Processing, and UCSD GPSA Interdisciplinary Research Award.
								</p>
							</div>
						</div>

						<!-- Speaker 2: Zhiyao Duan -->
						<div class="team-member">
							<a href="https://www.hajim.rochester.edu/ece/people/faculty/duan_zhiyao/index.html"
								class="image" target="_blank" rel="noopener noreferrer">
								<div class="team-photo">
									<img src="https://hajim.rochester.edu/ece/sites/zduan/resource/ZhiyaoDuan2018_web.jpg"
										alt="Zhiyao Duan" />
								</div>
							</a>
							<div class="team-info">
								<h2>
									<a href="https://www.hajim.rochester.edu/ece/people/faculty/duan_zhiyao/index.html"
										target="_blank" rel="noopener noreferrer">Zhiyao Duan</a>
								</h2>
								<p>
									Zhiyao Duan is an associate professor in Electrical and
									Computer Engineering, Computer Science, and Data Science at
									the University of Rochester. He is also a co-founder of
									Violy, a company aiming to improve music education through
									AI. His research interest is in computer audition and its
									connections with computer vision, natural language
									processing, and augmented and virtual reality. He received a
									best paper award at the Sound and Music Computing (SMC)
									Conference in 2017, a best paper nomination at the
									International Society for Music Information Retrieval
									(ISMIR) Conference in 2017, and a CAREER award from the
									National Science Foundation (NSF). His work has been funded
									by NSF, National Institute of Health, National Institute of
									Justice, New York State Center of Excellence in Data
									Science, and University of Rochester internal awards on
									AR/VR, health analytics, and data science. He is a senior
									area editor of IEEE Signal Processing Letters, an associate
									editor for IEEE Open Journal of Signal Processing, and a
									guest editor for Transactions of the International Society
									for Music Information Retrieval. He is the President of
									ISMIR.
								</p>
							</div>
						</div>

						<!-- Speaker 3: Miguel Willis -->
						<div class="team-member">
							<a href="https://www.law.upenn.edu/faculty/willism1" class="image" target="_blank"
								rel="noopener noreferrer">
								<div class="team-photo">
									<img src="https://www.law.upenn.edu/live/image/gid/35/width/518/height/576/crop/1/src_region/0,0,663,664/43301_Miguel_Willis.rev.1646392650.png"
										alt="Miguel Willis" />
								</div>
							</a>
							<div class="team-info">
								<h2>
									<a href="https://www.law.upenn.edu/faculty/willism1" target="_blank"
										rel="noopener noreferrer">Miguel Willis</a>
								</h2>
								<p>
									Miguel Willis is the Innovator in Residence at the Law
									School's Future of the Profession Initiative (FPI),
									University of Pennsylvania. He concurrently serves as the
									Executive Director of Access to Justice Tech Fellows, a
									national nonprofit organization that develops summer
									fellowships for law students seeking to leverage
									technology to create equitable legal access for low-income
									and marginalized populations. Prior to joining FPI, Willis
									served as the Law School Admissions Council's (LSAC)
									inaugural Presidential Innovation Fellow. Willis currently
									serves on the advisory board of the University of Arizona
									James E. Rogers College of Law's Innovation for Justice
									(i4J) program and serves on The Legal Services
									Corporation's Emerging Leaders Council.
								</p>
							</div>
						</div>

						<!-- Speaker 4: Gus Xia -->
						<div class="team-member">
							<a href="https://mbzuai.ac.ae/study/faculty/dr-gus-xia/" class="image" target="_blank"
								rel="noopener noreferrer">
								<div class="team-photo">
									<img src="https://mbzuai.ac.ae/wp-content/uploads/2022/06/profile_gus-xia_secondary.jpg"
										alt="Gus Xia" />
								</div>
							</a>
							<div class="team-info">
								<h2>
									<a href="https://mbzuai.ac.ae/study/faculty/dr-gus-xia/" target="_blank"
										rel="noopener noreferrer">Gus Xia</a>
								</h2>
								<p>
									Gus Xia is an assistant professor of Machine Learning at the
									Mohamed bin Zayed University of Artificial Intelligence in
									Masdar City, Abu Dhabi. His research includes the design of
									interactive intelligent systems to extend human musical
									creation and expression. This research lies at the
									intersection of machine learning, human-computer
									interaction, robotics, and computer music. Some
									representative works include interactive composition via
									style transfer, human-computer interactive performances,
									autonomous dancing robots, large-scale content-based music
									retrieval, haptic guidance for flute tutoring, and bio-music
									computing using slime mold.
								</p>
							</div>
						</div>
					</section>
				</section>

				<!-- Organizers -->
				<section>
					<h2>Organizers</h2>
					<p>
						Meet the team behind the 2025 AAAI Workshop on Artificial
						Intelligence for Music.
					</p>
					<section id="organizers" class="wrapper style1 spotlights">
						<!-- Organizer 1 -->
						<div class="team-member">
							<div class="team-photo">
								<img src="https://engineering.purdue.edu/ResourceDB/ResourceFiles/image277153"
									alt="Yung Hsiang Lu" class="profile-img" />
							</div>
							<div class="team-info">
								<h2>
									<a href="https://yhlu.net/" target="_blank" rel="noopener noreferrer">Yung-Hsiang
										Lu</a>
								</h2>
								<h3>Professor of Electrical and Computer Engineering</h3>
								<p>
									Yung-Hsiang Lu is a professor in the Elmore Family School of
									Electrical and Computer Engineering at Purdue University. He
									is a fellow of the IEEE and a distinguished scientist of the
									ACM. Yung-Hsiang has published papers on computer vision and
									machine learning in venues such as AI Magazine, Nature
									Machine Learning, and Computer. He is one of the editors of
									the book "Low-Power Computer Vision: Improve the Efficiency
									of Artificial Intelligence" (ISBN 9780367744700, 2022 by
									Chapman & Hall).
								</p>
							</div>
						</div>

						<!-- Organizer 2 -->
						<div class="team-member">
							<div class="team-photo">
								<img src="images/yun.jpg" alt="Dr. Kristen Yeon-Ji Yun" class="profile-img" />
							</div>
							<div class="team-info">
								<h2>
									<a href="https://kristenyeonjiyun.com/" target="_blank"
										rel="noopener noreferrer">Kristen Yeon-Ji Yun</a>
								</h2>
								<h3>Clinical Associate Professor of Music</h3>
								<p>
									Kristen Yeon-Ji Yun is a clinical associate professor in the
									Department of Music at the Patti and Rusty Rueff School of
									Design, Art, and Performance at Purdue University. She is
									the Principal Investigator of the research project
									"Artificial Intelligence Technology for Future Music
									Performers" (US National Science Foundation, IIS 2326198).
									Kristen is an active soloist, chamber musician, musical
									scholar, and clinician. She has toured many countries,
									including Malaysia, Thailand, Germany, Mexico, Japan, China,
									Hong Kong, Spain, France, Italy, Taiwan, and South Korea,
									giving a series of successful concerts and master classes.
								</p>
							</div>
						</div>

						<!-- Organizer 3 -->
						<div class="team-member">
							<div class="team-photo">
								<img src="https://www.luc.edu/media/lucedu/computerscience/images/newbwfacultyphotos/George_Thiruvathukal.jpg"
									alt="George K. Thiruvathukal" class="profile-img">
							</div>
							<div class="team-info">
								<h2>
									<a href="https://gkt.sh/" target="_blank" rel="noopener noreferrer">George K.
										Thiruvathukal</a>
								</h2>
								<h3>Professor and Chairperson of Computer Science</h3>
								<p>
									George K. Thiruvathukal is a professor and chairperson of
									Computer Science at Loyola University Chicago and a
									visiting computer scientist at Argonne National
									Laboratory. His research interests include
									high-performance computing and distributed systems,
									programming languages, software engineering, machine
									learning, digital humanities, and arts (primarily music).
									George has published multiple books, including "Software
									Engineering for Science" (ISBN 9780367574277, 2016 Chapman
									and Hall & CRC), "Web Programming: Techniques for
									Integrating Python, Linux, Apache, and MySQL" (ISBN
									9780130410658, 2001 Prentice Hall), and "High-Performance
									Java Platform Computing: Multithreaded and Networked
									Programming" (ISBN 9780130161642, 2000 Prentice Hall).
								</p>
							</div>
						</div>

						<!-- Organizer 4 -->
						<div class="team-member">
							<div class="team-photo">
								<img src="images/benchou.jpg" alt="Benjamin Shiue-Hal Chou" class="profile-img">
							</div>
							<div class="team-info">
								<h2>
									<a href="https://www.linkedin.com/in/benjamin-chou-6aa058228/" target="_blank"
										rel="noopener noreferrer">Benjamin Shiue-Hal Chou</a>
								</h2>
								<h3>PhD Student and Lab Graduate Mentor</h3>
								<p>
									Benjamin Shiue-Hal Chou is a PhD student in Electrical and
									Computer Engineering at Purdue University, supervised by
									Dr. Yung-Hsiang Lu. His research focuses on artificial
									intelligence applications in music technology,
									particularly on detecting errors in music performances.
									Benjamin has co-authored “Token Turing Machines are
									Efficient Vision Models” (arXiv preprint arXiv:2409.07613,
									2024). He earned his Bachelor of Science in Electrical
									Engineering from National Cheng Kung University (NCKU) in
									Taiwan, receiving awards such as the Outstanding Student
									Scholarship, Transnational Research Scholarship Grant, and
									the Tainan City Digital Governance Talent Award.
								</p>
							</div>
						</div>

						<!-- Add additional organizers following the same structure -->
					</section>
				</section>
			</div>
		</section>
	</div>

	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/jquery.scrollex.min.js"></script>
	<script src="assets/js/jquery.scrolly.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>
</body>

</html>
